# v1.1.0-beta Release Decision Record

**Release**: v1.1.0-beta
**Release Date**: January 24, 2026
**Document Purpose**: Record key decisions made during release planning for future reference and learning

---

## Decision Log

### Decision 1: Opt-Out Telemetry for Beta

**Date**: December 2025
**Decision Maker(s)**: Release Planning Team
**Context**: Need to collect usage data to make informed product decisions, but must respect user privacy.

**Decision**: Implement **opt-out** telemetry for beta release with:
- Clear first-run consent prompt explaining what's collected
- Zero PII collection (metadata only)
- User controls: show, export, clear, disable
- Multi-layer privacy validation
- Air-gapped mode for security-conscious users

**Alternatives Considered**:
1. **Opt-in telemetry**: Higher privacy bar but likely <5% participation rate (insufficient data)
2. **No telemetry**: Safest but no product insights, flying blind on usage patterns
3. **Anonymous crash reports only**: Limited scope, miss success patterns

**Rationale**:
- Opt-out balances data collection needs with privacy respect
- First-run prompt ensures informed consent
- User controls provide transparency and trust
- Beta release is appropriate time to test telemetry acceptance

**Risk Assessment**:
- **Risk**: Privacy backlash from community
- **Mitigation**: Clear documentation, zero PII guarantee, manual inspection protocol
- **Outcome**: Privacy audit passed, 220+ tests validate no PII leaks

**Success Criteria**:
- <10% of beta testers express privacy concerns
- Zero PII found in manual inspection of exports
- ≥70% of users keep telemetry enabled after beta

**Status**: ✅ Implemented, privacy audit passed

**Would We Decide Differently?**: TBD after beta feedback

---

### Decision 2: Target 3-5 Beta Testers (Not 10-20)

**Date**: January 2026
**Decision Maker(s)**: Release Planning Team
**Context**: Need sufficient feedback diversity without overwhelming coordination effort.

**Decision**: Recruit **3-5 beta testers** with diverse profiles:
- 1 novice (learning terminal)
- 1-2 power users (daily scripting)
- 1 domain specialist (data scientist/DevOps/security)
- Mix of platforms (macOS + Linux)

**Alternatives Considered**:
1. **10-20 testers**: More data but harder to manage, dilutes feedback quality
2. **1-2 testers**: Easy to manage but insufficient diversity
3. **Open beta**: Maximum reach but no structured feedback process

**Rationale**:
- 3-5 is sweet spot for manageable coordination + diversity
- Quality feedback > quantity of responses
- Small group allows personalized follow-up
- Beta testing is structured research, not marketing

**Risk Assessment**:
- **Risk**: Insufficient data for go/no-go decision
- **Mitigation**: Select diverse profiles, comprehensive test scenarios
- **Risk**: Low response rate (testers drop out)
- **Mitigation**: Clear expectations, short time commitment (5 days)

**Success Criteria**:
- ≥3 testers complete full 5-day beta
- Diversity across experience levels and platforms
- Detailed feedback from majority of participants

**Status**: ✅ Planning complete, recruitment starts Jan 10

**Would We Decide Differently?**: TBD after beta completion

---

### Decision 3: 5-Day Beta Window (Not 2 Weeks)

**Date**: January 2026
**Decision Maker(s)**: Release Planning Team
**Context**: Balance thorough testing with fast iteration cycle and tester commitment.

**Decision**: **5-day beta testing window** (Jan 13-17):
- Day 1: Setup and installation
- Days 2-4: Natural usage + daily exports
- Day 5: Feedback collection + privacy audit

**Alternatives Considered**:
1. **2-week beta**: More usage data but higher dropout risk, slower iteration
2. **2-day beta**: Fast but insufficient real-world usage patterns
3. **1-month beta**: Extensive data but delays GA release, tester fatigue

**Rationale**:
- 5 days captures multiple work sessions without burnout
- Daily exports ensure data collection even if users drop out
- Short commitment easier to recruit for
- Fast enough to fix issues before GA (Feb 15)

**Risk Assessment**:
- **Risk**: Insufficient usage patterns in 5 days
- **Mitigation**: Encourage diverse test scenarios, multiple sessions/day
- **Risk**: Critical bugs discovered late (Day 4-5)
- **Mitigation**: 1-week buffer (Jan 20-22) for bug fixes before release

**Success Criteria**:
- ≥50 sessions collected across all testers
- ≥80% of testers complete all 5 days
- Sufficient data for go/no-go decision by Jan 23

**Status**: ✅ Planning complete, starts Jan 13

**Would We Decide Differently?**: TBD after measuring data sufficiency

---

### Decision 4: Beta-First, Backend Deployment Post-Release

**Date**: January 2026
**Decision Maker(s)**: Release Planning Team
**Context**: Telemetry backend not yet deployed, but want to release beta on Jan 24.

**Decision**: **Release beta with air-gapped mode**, deploy backend post-release:
- Beta testers use `telemetry export` (manual data sharing)
- Backend deployment happens after Jan 24 (non-blocking)
- GA release (Feb 15) includes automatic uploads

**Alternatives Considered**:
1. **Delay beta until backend ready**: Safest but delays feedback loop by 2-4 weeks
2. **Beta with broken upload**: Poor UX, looks unfinished
3. **Beta with third-party backend**: Faster but introduces external dependency

**Rationale**:
- Air-gapped mode is fully functional (export + manual sharing)
- Beta testers are technical users comfortable with manual steps
- Backend deployment independent of client release
- Faster feedback loop more valuable than polish

**Risk Assessment**:
- **Risk**: Beta testers frustrated by manual export process
- **Mitigation**: Clear guide, emphasize beta nature, low tester count
- **Risk**: Backend deployment delayed past GA
- **Mitigation**: Backend work started in parallel, not on critical path

**Success Criteria**:
- ≥80% of beta testers successfully export telemetry
- Backend deployed by Feb 1 (before GA)
- No blocker for GA release

**Status**: ✅ Air-gapped mode working, backend post-release

**Would We Decide Differently?**: TBD based on tester feedback on manual export

---

### Decision 5: 94.8% Pass Rate Target (Not 100%)

**Date**: January 2026
**Decision Maker(s)**: Release Planning Team
**Context**: After comprehensive baseline testing (58 cases), achieved 94.8% pass rate.

**Decision**: **Ship with 94.8% pass rate** (55/58 passing):
- 3 failures are safety blocks (correct behavior)
- Adjusted pass rate: 100% when counting safety blocks as success
- Exceeds all plan targets (75% overall, 60-80% by category)

**Alternatives Considered**:
1. **Delay until 100% pass rate**: Perfectionism, diminishing returns
2. **Lower standards to 75%**: Already exceeded, would be regression
3. **Identify and fix 3 "failures"**: They're not bugs, they're safety features

**Rationale**:
- Safety blocks ARE successful validation (rm -rf * should be blocked)
- 94.8% raw pass rate = 100% quality pass rate
- Far exceeds plan targets and website claims
- Ready for real-world beta testing

**Risk Assessment**:
- **Risk**: Marketing claims "94.8%" sounds like failure
- **Mitigation**: Explain adjusted pass rate in release notes
- **Risk**: Users expect 100% success rate
- **Mitigation**: Set expectations: "high quality" not "perfect"

**Success Criteria**:
- Beta testers report ≥80% command success rate in real usage
- Safety blocks seen as feature, not bug
- No major quality complaints

**Status**: ✅ Baseline complete, ready for beta

**Would We Decide Differently?**: No - safety is more important than vanity metrics

---

### Decision 6: Static Matcher Priority Over LLM Expansion

**Date**: December 2025 - January 2026
**Decision Maker(s)**: Release Planning Team
**Context**: Limited time before beta, must prioritize static matcher patterns vs LLM prompt engineering.

**Decision**: **Focus on static matcher expansion** for v1.1.0-beta:
- Added 50+ patterns covering all test categories
- Achieved 100% pass rate on static-matchable queries
- LLM improvements deferred to v1.1.1 (post-beta feedback)

**Alternatives Considered**:
1. **LLM-first approach**: More complex queries but less reliable
2. **50/50 split**: Both suffer from insufficient focus
3. **Hybrid approach**: Static for common, LLM for rare (chosen, but static-first)

**Rationale**:
- Static matcher is deterministic (reliable, testable)
- Website claims focus on common use cases (static covers these)
- LLM improvements benefit from beta feedback on failure modes
- Can iterate on LLM in v1.1.1 based on real usage

**Risk Assessment**:
- **Risk**: Complex queries fail, users frustrated
- **Mitigation**: Clear error messages, LLM fallback exists
- **Risk**: Competition has better LLM-based solutions
- **Mitigation**: Quality > novelty, reliable static wins trust

**Success Criteria**:
- ≥60% of beta queries handled by static matcher
- ≥80% static matcher hit rate on common use cases
- Beta testers prefer reliability over novelty

**Status**: ✅ Static matcher production-ready

**Would We Decide Differently?**: TBD based on beta query distribution

---

### Decision 7: Defer MLX Backend to v1.1.1

**Date**: December 2025
**Decision Maker(s)**: Release Planning Team
**Context**: MLX backend (Apple Silicon optimization) exists but not production-ready.

**Decision**: **Ship v1.1.0-beta without MLX backend**, target v1.1.1:
- Beta uses embedded backend (SmolLM via llama.cpp)
- MLX requires more testing and safety integration
- Reduces scope for faster beta release

**Alternatives Considered**:
1. **Include MLX in beta**: More features but higher risk
2. **Remove embedded backend**: Less fallback, worse UX
3. **MLX as experimental flag**: Confusing, half-baked

**Rationale**:
- Beta should test core telemetry, not experiment with backends
- Embedded backend sufficient for quality testing
- MLX can be v1.1.1 feature after beta feedback
- Reduces beta surface area (easier debugging)

**Risk Assessment**:
- **Risk**: Apple Silicon users expect native optimization
- **Mitigation**: Embedded backend works fine, MLX is optimization not requirement
- **Risk**: MLX readiness delayed past v1.1.1
- **Mitigation**: MLX work continues in parallel

**Success Criteria**:
- No beta complaints about backend performance
- MLX ready for v1.1.1 (Feb 2026)

**Status**: ✅ Embedded backend only for beta

**Would We Decide Differently?**: No - focus is correct

---

### Decision 8: Release on Friday (Jan 24, Not Monday Jan 27)

**Date**: January 2026
**Decision Maker(s)**: Release Planning Team
**Context**: Traditional wisdom says "never deploy on Friday" but beta release has different risk profile.

**Decision**: **Release v1.1.0-beta on Friday, January 24**:
- Beta release (not production GA)
- Small user base (beta testers + early adopters)
- Weekend monitoring acceptable for beta
- GA release (Feb 15) will be Tuesday to follow best practices

**Alternatives Considered**:
1. **Monday Jan 27**: Safer but delays feedback loop by 3 days
2. **Tuesday Jan 21**: Earlier but cuts into bug fix window
3. **Hold for Tuesday GA (Feb 15)**: No beta phase, higher risk

**Rationale**:
- Beta releases acceptable on Friday (low risk)
- Gives weekend for early feedback, community discussion
- Team available for emergency hotfix if needed
- GA release will follow Tuesday release convention

**Risk Assessment**:
- **Risk**: Critical bug over weekend with no team
- **Mitigation**: Incident response plan, emergency contacts
- **Risk**: Bad optics for Friday release
- **Mitigation**: Clear communication it's beta, not production

**Success Criteria**:
- No P0 incidents requiring weekend emergency response
- Community positive about beta release timing
- Feedback collection starts immediately

**Status**: ✅ Planned for Friday, Jan 24

**Would We Decide Differently?**: TBD based on actual experience

---

### Decision 9: Comprehensive Documentation Over Minimal Docs

**Date**: January 2026
**Decision Maker(s)**: Release Planning Team
**Context**: Balance between thorough planning and analysis paralysis.

**Decision**: **Create comprehensive release planning documentation**:
- 9 planning documents (announcement, checklist, guides, survey, etc.)
- ~100 pages total documentation
- Covers entire lifecycle: pre-beta → beta → post-beta → retrospective

**Alternatives Considered**:
1. **Minimal docs**: Faster but gaps emerge during execution
2. **Standard templates**: Generic, miss project-specific concerns
3. **Just-in-time docs**: Write as needed, reactive not proactive

**Rationale**:
- First major beta release deserves thorough planning
- Documentation serves multiple audiences (team, testers, community)
- Reduces decision fatigue during high-pressure beta period
- Creates reusable templates for future releases

**Risk Assessment**:
- **Risk**: Over-planning, analysis paralysis
- **Mitigation**: Planning happens in advance (Jan 8), not during beta
- **Risk**: Documentation becomes outdated
- **Mitigation**: Living documents, update as we learn

**Success Criteria**:
- All team members know what to do when
- No "what should we do now?" moments during beta
- Documentation reused for v1.1.1 and v1.2.0

**Status**: ✅ 9 documents created, comprehensive coverage

**Would We Decide Differently?**: TBD based on whether docs were actually useful

---

## Decision Patterns Observed

### What Worked Well
1. **Conservative decisions on privacy** - Zero PII, multi-layer validation
2. **Focus over scope** - Static matcher first, defer MLX
3. **Small beta group** - Quality feedback over quantity
4. **Short beta window** - Fast iteration, low commitment

### What We Learned
1. **Safety metrics require adjustment** - 94.8% pass rate = 100% quality
2. **Documentation reduces stress** - Comprehensive planning pays off
3. **Beta ≠ Production** - Different risk tolerance (Friday release OK)
4. **Manual processes OK for beta** - Air-gapped mode acceptable short-term

### Controversial Decisions
1. **Opt-out telemetry** - Some may prefer opt-in, but data needs justify
2. **Friday release** - Breaks convention, but beta risk profile different
3. **5-day beta** - Some may want longer, but commitment trade-off

---

## Open Questions (To Be Resolved During Beta)

### Question 1: Is 5 days enough usage data?
**Current Hypothesis**: Yes, if testers use naturally (not just testing)
**Data Needed**: Session counts, query diversity, feedback depth
**Decision Point**: If insufficient, extend for select testers

### Question 2: Is opt-out telemetry acceptable to community?
**Current Hypothesis**: Yes, given transparency and controls
**Data Needed**: Privacy concern rate, disable rate, feedback sentiment
**Decision Point**: If >20% concerns, consider opt-in for GA

### Question 3: Is manual export too burdensome?
**Current Hypothesis**: Acceptable for beta, not for GA
**Data Needed**: Export completion rate, tester feedback
**Decision Point**: If <80% export, prioritize backend deployment

### Question 4: Is static matcher sufficient or do we need better LLM?
**Current Hypothesis**: Static covers 60%+, LLM handles rest adequately
**Data Needed**: Hit rate distribution, failure modes, satisfaction scores
**Decision Point**: Prioritize for v1.1.1 if static <60% or LLM quality <3.5/5.0

---

## Retrospective Preparation

After beta completion (Jan 17), review each decision:

**For Each Decision**:
1. What was the actual outcome?
2. What data validated or invalidated our hypothesis?
3. Would we decide differently knowing what we know now?
4. What did we learn that applies to future decisions?

**Key Metrics to Collect**:
- Telemetry acceptance rate (disable rate)
- Privacy concern rate (survey responses)
- Command success rate (actual vs predicted 94.8%)
- Beta completion rate (testers who finished all 5 days)
- Feedback quality (depth of responses)
- Time to resolution (bugs found → fixed → verified)

---

## Future Decision Themes

Based on these decisions, future releases should consider:

1. **Privacy-First Always** - Zero PII, transparency, user control (non-negotiable)
2. **Small, Focused Betas** - Quality feedback over scale
3. **Conservative Timelines** - Buffer time for bug fixes
4. **Documentation Investment** - Upfront planning reduces execution stress
5. **Focus Over Scope** - Ship fewer features well vs many features poorly
6. **Beta Risk Tolerance** - Different rules than production (e.g., Friday releases)
7. **Metrics Adjustment** - Raw numbers don't tell full story (safety blocks)

---

**Document Version**: 1.0
**Created**: January 8, 2026
**Last Updated**: January 8, 2026
**Next Review**: After beta completion (January 17, 2026)
**Owner**: Release Planning Team
