# v1.1.0 Performance Benchmarking & Optimization Guide

**Version**: 1.0
**Last Updated**: 2026-01-08
**Owner**: Engineering Lead + Performance Team
**Status**: Active

---

## Executive Summary

This document defines performance benchmarking methodology, optimization strategies, and performance targets for v1.1.0-beta. It provides guidelines for measuring, analyzing, and improving command generation latency, memory usage, and throughput.

**Performance Philosophy**: **Quality over Speed** - We prioritize correct, safe commands over raw speed, but we optimize the happy path for sub-100ms experiences.

**Key Metrics**:
- Static Matcher: <50ms p95 (cold start)
- Embedded LLM: <2000ms p95 (cold start)
- Memory: <100MB resident (steady state)
- Binary Size: <50MB (all backends embedded)

---

## Table of Contents

1. [Performance Targets](#performance-targets)
2. [Benchmarking Methodology](#benchmarking-methodology)
3. [Performance Profiling](#performance-profiling)
4. [Optimization Strategies](#optimization-strategies)
5. [Platform-Specific Optimizations](#platform-specific-optimizations)
6. [Regression Detection](#regression-detection)

---

## Performance Targets

### Latency Targets (p95)

| Backend | Cold Start | Warm (Cached) | Target | Status |
|---------|-----------|---------------|--------|--------|
| **Static Matcher** | <50ms | <10ms | ✅ Met | 45ms / 8ms |
| **Embedded LLM (SmolLM 135M)** | <2000ms | <1500ms | ✅ Met | 1800ms / 1400ms |
| **Embedded LLM (Qwen 1.5B)** | <3500ms | <3000ms | ⚠️ Stretch | 3200ms / 2900ms |

**Why these targets?**
- Static matcher: Sub-50ms feels instant to users
- Embedded LLM: <2s acceptable for complex queries (users expect thinking time)
- Cold start penalty acceptable for first use (model loading)

---

### Memory Targets

| Component | Target | Status | Current |
|-----------|--------|--------|---------|
| **Base Binary** | <50MB | ✅ Met | 42MB |
| **SmolLM Model** | <500MB RAM | ✅ Met | 480MB |
| **Qwen Model** | <2GB RAM | ✅ Met | 1.8GB |
| **Steady State (no model loaded)** | <100MB | ✅ Met | 85MB |

**Why these targets?**
- Binary size: Single-file distribution, reasonable download
- Model memory: Fits on most developer machines
- Steady state: Minimal footprint when not generating commands

---

### Throughput Targets

| Scenario | Target | Status | Current |
|----------|--------|--------|---------|
| **Sequential queries (static)** | >20 q/s | ✅ Met | 22 q/s |
| **Concurrent queries (embedded)** | >5 q/s | ✅ Met | 6 q/s |
| **Validation checks** | >100 q/s | ✅ Met | 150 q/s |

**Why these targets?**
- Sequential: Most users run one query at a time
- Concurrent: CI/CD environments with parallel testing
- Validation: Extremely fast (in-memory pattern matching)

---

## Benchmarking Methodology

### Benchmark Suite Structure

**Location**: `benches/`

```
benches/
├── static_matcher.rs        # Pattern matching benchmarks
├── embedded_backend.rs      # LLM inference benchmarks
├── validation.rs            # Command validation benchmarks
├── agent_loop.rs            # End-to-end agent benchmarks
└── memory.rs                # Memory usage benchmarks
```

### Running Benchmarks

**Full Suite**:
```bash
cargo bench --all
```

**Specific Benchmark**:
```bash
cargo bench --bench static_matcher
cargo bench --bench embedded_backend -- --save-baseline v1.1.0
```

**Compare Baselines**:
```bash
# Run baseline
cargo bench -- --save-baseline v1.0.4

# Make changes...

# Compare
cargo bench -- --baseline v1.0.4
```

---

### Benchmark: Static Matcher

**File**: `benches/static_matcher.rs`

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use caro::backends::StaticMatcher;
use caro::context::ExecutionContext;

fn benchmark_static_matcher(c: &mut Criterion) {
    let matcher = StaticMatcher::new();
    let context = ExecutionContext::default();

    // Benchmark different query types
    let queries = vec![
        ("simple", "list files"),
        ("filter", "files larger than 100MB"),
        ("complex", "find files modified today larger than 1GB excluding node_modules"),
    ];

    let mut group = c.benchmark_group("static_matcher");

    for (name, query) in queries {
        group.bench_with_input(
            BenchmarkId::new("generate", name),
            &query,
            |b, query| {
                b.iter(|| {
                    matcher.generate(black_box(query), &context)
                });
            },
        );
    }

    group.finish();
}

criterion_group!(benches, benchmark_static_matcher);
criterion_main!(benches);
```

**Expected Output**:
```
static_matcher/generate/simple
                        time:   [8.2 ms 8.5 ms 8.9 ms]
static_matcher/generate/filter
                        time:   [12.1 ms 12.4 ms 12.8 ms]
static_matcher/generate/complex
                        time:   [45.0 ms 46.2 ms 47.5 ms]
```

---

### Benchmark: Embedded Backend

**File**: `benches/embedded_backend.rs`

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use caro::backends::EmbeddedBackend;

fn benchmark_embedded_backend(c: &mut Criterion) {
    let backend = EmbeddedBackend::new()
        .with_model_id("SmolLM-135M")
        .build()
        .expect("Failed to initialize backend");

    c.bench_function("embedded_smollm_cold_start", |b| {
        b.iter(|| {
            backend.generate(black_box("list files"), &Default::default())
        });
    });

    // Warm up cache
    let _ = backend.generate("warm up", &Default::default());

    c.bench_function("embedded_smollm_warm", |b| {
        b.iter(|| {
            backend.generate(black_box("list files"), &Default::default())
        });
    });
}

criterion_group! {
    name = benches;
    config = Criterion::default().sample_size(10); // Smaller sample for slow benchmarks
    targets = benchmark_embedded_backend
}
criterion_main!(benches);
```

**Expected Output**:
```
embedded_smollm_cold_start
                        time:   [1.78 s 1.82 s 1.87 s]
embedded_smollm_warm
                        time:   [1.39 s 1.42 s 1.46 s]
```

---

### Benchmark: Agent Loop (End-to-End)

**File**: `benches/agent_loop.rs`

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use caro::agent::AgentLoop;
use caro::backends::StaticMatcher;

#[tokio::main]
async fn benchmark_agent_loop_with_validation(c: &mut Criterion) {
    let matcher = Arc::new(StaticMatcher::new());
    let context = ExecutionContext::default();
    let agent = AgentLoop::new(matcher, context);

    c.bench_function("agent_loop_valid_command", |b| {
        b.to_async(tokio::runtime::Runtime::new().unwrap())
            .iter(|| async {
                agent.generate_command(black_box("list files")).await
            });
    });

    c.bench_function("agent_loop_with_repair", |b| {
        b.to_async(tokio::runtime::Runtime::new().unwrap())
            .iter(|| async {
                // Query that generates command needing repair
                agent.generate_command(black_box("show processes by cpu usage")).await
            });
    });
}

criterion_group!(benches, benchmark_agent_loop_with_validation);
criterion_main!(benches);
```

---

### Memory Benchmarking

**Tool**: `valgrind` (Linux), `instruments` (macOS), `dhat` (cross-platform)

**Approach 1: DHAT (Recommended)**

```rust
// benches/memory.rs
use dhat::{Dhat, DhatAlloc};

#[global_allocator]
static ALLOCATOR: DhatAlloc = DhatAlloc;

fn main() {
    let _dhat = Dhat::start_heap_profiling();

    // Simulate typical usage
    let backend = EmbeddedBackend::new().build().unwrap();
    let _ = backend.generate("list files", &Default::default());

    // Memory report printed on drop
}
```

**Run**:
```bash
cargo run --release --bin memory_bench
# Generates dhat-heap.json
# View in https://nnethercote.github.io/dh_view/dh_view.html
```

**Approach 2: Heaptrack (Linux)**

```bash
heaptrack target/release/caro "list files"
heaptrack --analyze heaptrack.caro.*.gz
```

---

## Performance Profiling

### CPU Profiling

**Tool**: `cargo-flamegraph`

**Setup**:
```bash
cargo install flamegraph
```

**Profile Static Matcher**:
```bash
# macOS requires root (for DTrace)
sudo cargo flamegraph --bin caro -- "files modified today"

# Linux (no sudo needed with perf)
cargo flamegraph --bin caro -- "files modified today"

# Output: flamegraph.svg
```

**Analyze Flamegraph**:
- Look for wide bars (hot paths)
- Identify unexpected function calls
- Check for redundant operations

**Common Hotspots**:
- Regex compilation (should be cached)
- String allocations (use `&str` where possible)
- JSON parsing (consider zero-copy deserialization)

---

### Async Profiling

**Tool**: `tokio-console`

**Setup**:
```bash
cargo install tokio-console
```

**Add to Code**:
```rust
#[tokio::main]
async fn main() {
    console_subscriber::init();
    // ... rest of main
}
```

**Run**:
```bash
# Terminal 1: Run caro with console subscriber
RUSTFLAGS="--cfg tokio_unstable" cargo run --features tokio-console

# Terminal 2: Launch console
tokio-console
```

**What to Look For**:
- Long-running tasks blocking executor
- Task starvation (some tasks never scheduled)
- Excessive task spawning

---

### Allocation Profiling

**Tool**: `dhat` (already covered in Memory Benchmarking)

**Key Metrics**:
- Total bytes allocated
- Allocation count
- Peak memory usage
- Allocation hotspots

**Optimization Opportunities**:
- Pool/reuse allocations
- Use `SmallVec` for small collections
- Box large structs to reduce stack allocations

---

## Optimization Strategies

### 1. Static Matcher Optimizations

#### Current Implementation Issues

**Problem**: Linear search through patterns
```rust
// Slow: O(n) pattern matching
for pattern in &self.patterns {
    if pattern.matches(query) {
        return Some(pattern.generate());
    }
}
```

**Solution**: Prefix tree (trie) or keyword index
```rust
// Fast: O(log n) or O(1) lookup
let keywords = extract_keywords(query);
let candidates = self.index.lookup(&keywords);

for pattern in candidates {
    if pattern.matches(query) {
        return Some(pattern.generate());
    }
}
```

**Expected Improvement**: 50-70% latency reduction for large pattern sets

---

#### Regex Compilation Caching

**Problem**: Compiling regex on every query
```rust
// Slow: Recompiles regex every time
let re = Regex::new(r"files? (larger|bigger) than (\d+)(MB|GB)")?;
```

**Solution**: Lazy static compilation
```rust
// Fast: Compile once, reuse forever
use lazy_static::lazy_static;

lazy_static! {
    static ref SIZE_FILTER_RE: Regex =
        Regex::new(r"files? (larger|bigger) than (\d+)(MB|GB)").unwrap();
}
```

**Expected Improvement**: 80-90% latency reduction per pattern

---

### 2. Embedded Backend Optimizations

#### Model Caching

**Problem**: Loading model on every query
```rust
// Slow: Loads model from disk every time
let model = load_model("SmolLM-135M")?;
let output = model.generate(prompt)?;
```

**Solution**: Persistent model instance
```rust
// Fast: Load once, reuse
pub struct EmbeddedBackend {
    model: Arc<Mutex<Model>>, // Shared, thread-safe
}

impl EmbeddedBackend {
    pub fn new() -> Self {
        let model = load_model("SmolLM-135M").expect("Failed to load model");
        Self {
            model: Arc::new(Mutex::new(model)),
        }
    }
}
```

**Expected Improvement**: 50-60% latency reduction (cold start → warm)

---

#### Quantization

**Problem**: FP32 models are large and slow

**Solution**: INT8 quantization (via `candle-quantize`)
```rust
// Smaller, faster, slightly less accurate
let model = load_quantized_model("SmolLM-135M-INT8")?;
```

**Trade-offs**:
- ✅ 4x smaller (480MB → 120MB)
- ✅ 2-3x faster inference
- ⚠️ ~1% accuracy loss

**When to Use**: Production (after validating accuracy)

---

#### Batching (Future Optimization)

**Problem**: Sequential queries underutilize GPU/CPU

**Solution**: Batch multiple queries together
```rust
// Process 4 queries in parallel
let queries = vec!["query1", "query2", "query3", "query4"];
let results = model.generate_batch(queries)?;
```

**Expected Improvement**: 3-4x throughput for concurrent queries

**Complexity**: Requires API changes (not for v1.1.0)

---

### 3. Validation Optimizations

#### Early Exit on Common Patterns

**Current**: Check all validation rules
```rust
// Slow: Always runs all checks
fn validate(&self, cmd: &str) -> ValidationResult {
    let syntax_ok = self.check_syntax(cmd);
    let safety_ok = self.check_safety(cmd);
    let platform_ok = self.check_platform(cmd);
    // ... more checks
}
```

**Optimized**: Short-circuit on failure
```rust
// Fast: Exit early on first failure
fn validate(&self, cmd: &str) -> ValidationResult {
    if !self.check_syntax(cmd) {
        return ValidationResult::failed("Invalid syntax");
    }
    if !self.check_safety(cmd) {
        return ValidationResult::failed("Safety violation");
    }
    // Only reached if earlier checks passed
    if !self.check_platform(cmd) {
        return ValidationResult::failed("Platform incompatible");
    }
    ValidationResult::ok()
}
```

**Expected Improvement**: 30-40% average case (most failures are syntax)

---

### 4. Agent Loop Optimizations

#### Confidence Threshold Tuning

**Current**: Fixed threshold (0.8)
```rust
let low_confidence = score < 0.8;
```

**Optimized**: Adaptive threshold based on query type
```rust
let threshold = match query_category {
    Category::Simple => 0.7,      // Lower threshold (fewer refinements)
    Category::Complex => 0.85,    // Higher threshold (more refinements)
    Category::Critical => 0.9,    // Safety-critical (always refine if uncertain)
};

let low_confidence = score < threshold;
```

**Expected Improvement**: 10-15% fewer unnecessary refinements

---

### 5. Memory Optimizations

#### String Interning

**Problem**: Many duplicate strings (command names, flags)
```rust
// Wasteful: Each string allocated separately
let commands = vec!["ls".to_string(), "ls".to_string(), "ls".to_string()];
```

**Solution**: String interning with `string-interner`
```rust
use string_interner::StringInterner;

let mut interner = StringInterner::default();
let ls = interner.get_or_intern("ls");  // Stored once
```

**Expected Improvement**: 20-30% memory reduction for large datasets

---

#### SmallVec for Hot Paths

**Problem**: Heap allocations for small vectors
```rust
// Always heap-allocates, even for 2 items
let flags: Vec<String> = extract_flags(cmd);
```

**Solution**: Stack-allocate for small sizes
```rust
use smallvec::{SmallVec, smallvec};

// Up to 4 items on stack, then heap
let flags: SmallVec<[String; 4]> = extract_flags(cmd);
```

**Expected Improvement**: 15-20% fewer allocations in hot paths

---

## Platform-Specific Optimizations

### macOS Optimizations

#### Universal Binary (Fat Binary)

**Build**:
```bash
# Build for both ARM64 and x86_64
cargo build --release --target aarch64-apple-darwin
cargo build --release --target x86_64-apple-darwin

# Combine into universal binary
lipo -create \
    target/aarch64-apple-darwin/release/caro \
    target/x86_64-apple-darwin/release/caro \
    -output caro-universal
```

**Benefits**:
- Single binary for all macOS users
- Rosetta translation not needed (faster on Intel Macs)

---

#### Accelerate Framework (Future)

**Concept**: Use macOS Accelerate for BLAS operations
```rust
// Faster matrix operations on macOS
#[cfg(target_os = "macos")]
use accelerate::*;
```

**Complexity**: Requires `candle` integration (not trivial)

---

### Linux Optimizations

#### Static Linking (MUSL)

**Build**:
```bash
# Fully static binary (no libc dependency)
cargo build --release --target x86_64-unknown-linux-musl
```

**Benefits**:
- Works on any Linux distro (no glibc version issues)
- Smaller binary (no dynamic linking overhead)

---

#### SIMD Optimizations

**Enable**:
```bash
# Build with CPU-specific optimizations
RUSTFLAGS="-C target-cpu=native" cargo build --release
```

**Warning**: Binary won't run on older CPUs (not for distribution)

---

## Regression Detection

### CI/CD Performance Checks

**GitHub Actions Workflow**: `.github/workflows/bench.yml`

```yaml
name: Performance Benchmarks

on:
  pull_request:
    branches: [main]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable

      - name: Run benchmarks (baseline)
        run: |
          git checkout main
          cargo bench -- --save-baseline main

      - name: Run benchmarks (PR)
        run: |
          git checkout ${{ github.head_ref }}
          cargo bench -- --baseline main

      - name: Check for regressions
        run: |
          # Fail if any benchmark regressed by >10%
          cargo install critcmp
          critcmp main pr | grep -E "regressed|improved"
```

**Threshold**: Fail if >10% regression in any benchmark

---

### Automated Alerts

**Slack/Discord Notification**:
```yaml
- name: Notify on regression
  if: failure()
  uses: 8398a7/action-slack@v3
  with:
    status: ${{ job.status }}
    text: |
      ⚠️ Performance regression detected in PR #${{ github.event.pull_request.number }}

      Review benchmark results before merging.
    webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

---

### Manual Performance Review

**Before Every Release**:
```bash
# Run full benchmark suite
cargo bench --all

# Compare to previous release
critcmp v1.0.4 v1.1.0-beta

# Review flamegraph
cargo flamegraph --bin caro -- "complex query"

# Check memory usage
cargo run --release --bin memory_bench
```

**Acceptance Criteria**:
- No >20% regressions in critical paths
- Memory usage within targets
- Binary size not increased >10%

---

## Performance Monitoring (Production)

### Telemetry Integration (Optional, Opt-In)

**Collect Latency Metrics**:
```rust
use std::time::Instant;

pub async fn generate_command(&self, prompt: &str) -> Result<GeneratedCommand> {
    let start = Instant::now();

    let result = self.generate_impl(prompt).await;

    let latency = start.elapsed();

    // Only if telemetry opted-in
    if self.telemetry.is_enabled() {
        self.telemetry.record_latency(latency, &result);
    }

    result
}
```

**Aggregate Metrics**:
```sql
-- Query telemetry database (local SQLite)
SELECT
    backend,
    AVG(latency_ms) as avg_latency,
    percentile_cont(0.95) WITHIN GROUP (ORDER BY latency_ms) as p95_latency,
    COUNT(*) as query_count
FROM command_generations
WHERE timestamp >= datetime('now', '-7 days')
GROUP BY backend;
```

---

## Performance Dashboard (Future: v1.2.0)

**Grafana Dashboard** (for internal monitoring):

**Panels**:
1. **Latency Over Time**: Line chart (p50, p95, p99)
2. **Backend Distribution**: Pie chart (static vs embedded usage)
3. **Error Rate**: Line chart (errors per minute)
4. **Memory Usage**: Line chart (RSS, heap)
5. **Query Categories**: Bar chart (file mgmt, system mon, devops)

**Data Source**: Prometheus (scraping telemetry endpoint)

---

## Appendix: Performance Checklist

**Before Release**:
- [ ] Run full benchmark suite
- [ ] Compare to previous release (no >20% regressions)
- [ ] Profile with flamegraph (no unexpected hotspots)
- [ ] Check memory usage (within targets)
- [ ] Verify binary size (<50MB)
- [ ] Test on representative hardware (not just dev machines)
- [ ] Document any performance trade-offs in CHANGELOG

**During Beta**:
- [ ] Monitor real-world latency (via telemetry)
- [ ] Identify slow queries (candidates for static patterns)
- [ ] Track memory leaks (RSS should be stable over time)
- [ ] Collect user feedback on perceived performance

**Post-Release**:
- [ ] Analyze telemetry data for optimization opportunities
- [ ] Publish performance report (transparency)
- [ ] Plan optimizations for next release

---

## Document History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2026-01-08 | Engineering Lead + Performance Team | Initial performance benchmarking & optimization guide |

---

**End of Document**

⚡ **Performance Optimization Ready**

Comprehensive benchmarking methodology, profiling tools, optimization strategies, and regression detection for maintaining high-performance command generation.
