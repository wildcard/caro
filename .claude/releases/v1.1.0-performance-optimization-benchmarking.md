# Release Performance Optimization & Benchmarking Strategy

**Version**: 1.0
**Last Updated**: 2026-01-08
**Owner**: Engineering Lead + Performance Engineer
**Status**: Active

---

## Document Purpose

This document defines the comprehensive performance optimization and benchmarking strategy for the v1.1.0-beta release, ensuring caro delivers fast, responsive command generation across all platforms and backends. It establishes performance targets, measurement methodology, optimization priorities, and continuous monitoring.

**Audience**: Engineering Team, Performance Engineer, Release Manager, Product Lead

**Related Documents**:
- `v1.1.0-release-metrics-dashboard-monitoring.md` - Performance metrics in dashboard
- `v1.1.0-release-qa-testing-strategy.md` - Performance testing as part of QA
- `v1.1.0-technical-debt-code-health.md` - Performance debt management
- `ARCHITECTURE.md` - System architecture and design decisions

---

## Table of Contents

1. [Performance Philosophy](#performance-philosophy)
2. [Performance Targets](#performance-targets)
3. [Measurement Methodology](#measurement-methodology)
4. [Benchmarking Framework](#benchmarking-framework)
5. [Optimization Priorities](#optimization-priorities)
6. [Backend-Specific Optimization](#backend-specific-optimization)
7. [Platform-Specific Optimization](#platform-specific-optimization)
8. [Continuous Performance Monitoring](#continuous-performance-monitoring)
9. [Performance Regression Prevention](#performance-regression-prevention)

---

## Performance Philosophy

### Core Principles

**1. Performance is a Feature**
- Fast response times are not optional (users expect instant results)
- Performance directly impacts user satisfaction and retention
- Every 100ms delay reduces user engagement
- "Fast enough" is not good enough (always optimize)

**2. Measure, Don't Guess**
- All optimization decisions driven by data (profiling, benchmarks, telemetry)
- No premature optimization (profile first, then optimize hotspots)
- Quantify impact (before/after benchmarks for every optimization)
- Regression testing (detect performance regressions in CI/CD)

**3. Optimize for Common Case**
- 80/20 rule: Optimize the 80% use case first
- Static matcher handles most queries (<100ms)
- Embedded backend for complex queries (<500ms)
- Ollama backend for advanced use cases (<1000ms)

**4. User-Perceived Performance > Raw Throughput**
- Latency (p50, p95, p99) matters more than throughput
- Tail latency (p99) is critical (worst user experience)
- Startup time matters (first command should be instant)
- Progressive loading (show results as they arrive, not after complete generation)

**5. Platform-Aware Optimization**
- Optimize for target platforms (macOS Intel, macOS Apple Silicon, Linux x86_64, Linux ARM)
- Leverage platform-specific features (MLX on Apple Silicon, AVX on x86_64)
- Respect resource constraints (embedded backend uses <100MB RAM)

---

## Performance Targets

### Latency Targets (p95)

| Backend | Simple Query | Medium Query | Complex Query | Target |
|---------|--------------|--------------|---------------|--------|
| **Static Matcher** | <50ms | <100ms | N/A | <100ms p95 |
| **Embedded (SmolLM)** | <300ms | <500ms | <800ms | <500ms p95 |
| **Ollama (External)** | <500ms | <1000ms | <2000ms | <1000ms p95 |

**Query Complexity Definitions**:
- **Simple**: Single-step operation, clear intent (e.g., "list files")
- **Medium**: Multi-condition filtering, moderate complexity (e.g., "find large log files modified today")
- **Complex**: Multi-step operations, context-dependent (e.g., "show disk usage for all docker containers")

---

### Throughput Targets

| Backend | Queries/Second (Single User) | Concurrent Users (System-Wide) | Target |
|---------|-------------------------------|--------------------------------|--------|
| **Static Matcher** | 50+ qps | 100+ users | Essentially unlimited (pattern matching) |
| **Embedded** | 5-10 qps | 20-30 users | CPU-bound, scales with cores |
| **Ollama** | 1-5 qps | 10-20 users | Network-bound, depends on server |

**Note**: Throughput targets for v1.1.0-beta (150 AWU target). Scale to 1,000+ users in v1.2.0+.

---

### Resource Utilization Targets

| Resource | Static Matcher | Embedded Backend | Ollama Backend | Target |
|----------|----------------|------------------|----------------|--------|
| **Memory (Baseline)** | <20MB | <100MB | <30MB | <100MB total |
| **Memory (Peak)** | <50MB | <250MB | <50MB | <300MB total |
| **CPU (Idle)** | 0% | 0% | 0% | 0% (CLI should idle at 0%) |
| **CPU (Active)** | <5% | <50% (1 core) | <5% | Minimal blocking |
| **Disk I/O** | <1MB/query | <5MB/query (model load) | <1MB/query | Low I/O |
| **Network** | 0 | 0 | <10KB/query | Only Ollama uses network |

**Target**: Caro should be lightweight (no background processes, no resource hogging)

---

### Startup Time Targets

| Phase | Target | Rationale |
|-------|--------|-----------|
| **Cold Start** (first command after install) | <500ms | Initial model load (embedded), acceptable for first use |
| **Warm Start** (subsequent commands) | <100ms | Model already in memory, should be instant |
| **Model Loading** (embedded backend) | <300ms | One-time cost, amortized over session |

**Key Metric**: Time from `caro "query"` to command output displayed

---

## Measurement Methodology

### Benchmarking Environment

**Hardware Profiles**:

**Profile 1: macOS Apple Silicon (Primary Target)**
- CPU: Apple M1/M2/M3 (8-core)
- RAM: 16GB
- Storage: SSD
- OS: macOS 13+ (Ventura)

**Profile 2: macOS Intel (Legacy)**
- CPU: Intel Core i7 (4-core, 2.8GHz)
- RAM: 16GB
- Storage: SSD
- OS: macOS 12+ (Monterey)

**Profile 3: Linux x86_64 (Server/Cloud)**
- CPU: AMD Ryzen / Intel Xeon (4-8 core)
- RAM: 8-16GB
- Storage: SSD
- OS: Ubuntu 22.04 / Debian 12

**Profile 4: Linux ARM (Raspberry Pi)**
- CPU: ARM Cortex (4-core)
- RAM: 4-8GB
- Storage: microSD (slower I/O)
- OS: Ubuntu 22.04 ARM

---

### Benchmark Methodology

**1. Micro-Benchmarks** (Isolated Component Performance)
- **Tool**: `cargo bench` (Criterion.rs)
- **Focus**: Individual functions (pattern matching, tokenization, safety validation)
- **Frequency**: Every commit (CI/CD)
- **Threshold**: >10% regression fails CI

**Example Micro-Benchmark**:
```rust
// benches/safety_validation.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use caro::safety::SafetyValidator;

fn bench_safety_validation(c: &mut Criterion) {
    let validator = SafetyValidator::new();

    c.bench_function("validate safe command", |b| {
        b.iter(|| {
            validator.validate(black_box("ls -la"))
        })
    });

    c.bench_function("validate dangerous command", |b| {
        b.iter(|| {
            validator.validate(black_box("rm -rf /"))
        })
    });
}

criterion_group!(benches, bench_safety_validation);
criterion_main!(benches);
```

---

**2. End-to-End Benchmarks** (Full Pipeline Performance)
- **Tool**: Custom benchmark suite (`benches/e2e_benchmarks.rs`)
- **Focus**: Complete flow (query → backend → validation → output)
- **Frequency**: Pre-release (Week 1, Day 6)
- **Metrics**: p50, p95, p99 latency

**Example E2E Benchmark**:
```rust
// benches/e2e_benchmarks.rs
use caro::Caro;
use std::time::Instant;

#[bench]
fn bench_static_matcher_simple_query() {
    let caro = Caro::new();
    let query = "list files";

    let start = Instant::now();
    let result = caro.generate(query).unwrap();
    let duration = start.elapsed();

    assert!(duration < Duration::from_millis(100));  // <100ms target
    assert_eq!(result.command, "ls");
}

#[bench]
fn bench_embedded_backend_complex_query() {
    let caro = Caro::new_with_backend(Backend::Embedded);
    let query = "find all log files larger than 10MB modified in the last 7 days";

    let start = Instant::now();
    let result = caro.generate(query).unwrap();
    let duration = start.elapsed();

    assert!(duration < Duration::from_millis(500));  // <500ms target
}
```

---

**3. Load Testing** (System-Wide Throughput)
- **Tool**: Custom load test harness (simulate concurrent users)
- **Focus**: System behavior under load (throughput, resource usage)
- **Frequency**: Pre-release (Week 1, Day 6)
- **Metrics**: Queries/second, CPU/memory usage, error rates

**Example Load Test**:
```rust
// tests/load_test.rs
use std::thread;
use std::sync::Arc;
use caro::Caro;

#[test]
fn load_test_embedded_backend() {
    let caro = Arc::new(Caro::new_with_backend(Backend::Embedded));
    let num_threads = 10;  // Simulate 10 concurrent users
    let queries_per_thread = 100;

    let handles: Vec<_> = (0..num_threads)
        .map(|_| {
            let caro_clone = Arc::clone(&caro);
            thread::spawn(move || {
                for _ in 0..queries_per_thread {
                    let _ = caro_clone.generate("list files");
                }
            })
        })
        .collect();

    let start = Instant::now();
    for handle in handles {
        handle.join().unwrap();
    }
    let duration = start.elapsed();

    let total_queries = num_threads * queries_per_thread;
    let qps = total_queries as f64 / duration.as_secs_f64();

    println!("Throughput: {:.2} qps", qps);
    assert!(qps > 50.0);  // Target: >50 qps under load
}
```

---

**4. Profiling** (Identify Bottlenecks)
- **Tool**: `cargo flamegraph` (CPU profiling), `valgrind --tool=massif` (memory profiling)
- **Focus**: Hotspots (which functions consume most time/memory?)
- **Frequency**: Ad-hoc (when investigating performance issues)
- **Output**: Flamegraph SVG, memory usage timeline

**Example Profiling Workflow**:
```bash
# CPU Profiling (Flamegraph)
cargo flamegraph --bin caro -- "find large files"
# Output: flamegraph.svg (visual representation of CPU hotspots)

# Memory Profiling (Massif)
valgrind --tool=massif --massif-out-file=massif.out ./target/release/caro "find large files"
ms_print massif.out
# Output: Memory usage timeline, peak allocations
```

---

## Benchmarking Framework

### Benchmark Suite Structure

```
benches/
├── safety_validation.rs      # Micro-benchmarks: Safety module
├── pattern_matching.rs        # Micro-benchmarks: Static matcher
├── tokenization.rs            # Micro-benchmarks: LLM tokenization
├── inference.rs               # Micro-benchmarks: LLM inference (embedded)
├── e2e_static.rs              # E2E: Static matcher pipeline
├── e2e_embedded.rs            # E2E: Embedded backend pipeline
├── e2e_ollama.rs              # E2E: Ollama backend pipeline
└── load_test.rs               # Load testing: Concurrent users
```

---

### Benchmark Execution

**CI/CD Benchmarks** (Every Commit):
```bash
# Run micro-benchmarks (fast, <5 min)
cargo bench --bench safety_validation
cargo bench --bench pattern_matching

# Detect regressions (fail if >10% slower)
cargo bench -- --save-baseline main
cargo bench -- --baseline main
```

**Pre-Release Benchmarks** (Week 1, Day 6):
```bash
# Run full benchmark suite (30-60 min)
cargo bench

# E2E benchmarks across all backends
cargo test --release --test e2e_static
cargo test --release --test e2e_embedded
cargo test --release --test e2e_ollama

# Load testing (simulate 100 concurrent users)
cargo test --release --test load_test -- --nocapture
```

**Manual Profiling** (Ad-Hoc):
```bash
# CPU profiling
cargo flamegraph --bin caro -- "complex query"

# Memory profiling
valgrind --tool=massif ./target/release/caro "complex query"

# System-level profiling (macOS)
instruments -t "Time Profiler" ./target/release/caro "complex query"
```

---

### Benchmark Reporting

**Format**: Markdown table with baseline comparison

**Example Report** (`.claude/releases/performance/benchmark-report-2026-01-12.md`):

```markdown
# Performance Benchmark Report: v1.1.0-beta

**Date**: 2026-01-12
**Platform**: macOS M1 (8-core, 16GB RAM)
**Baseline**: v1.0.0

## Latency (p95)

| Backend | Query Type | v1.0.0 | v1.1.0-beta | Change | Target | Status |
|---------|------------|--------|-------------|--------|--------|--------|
| Static | Simple | 45ms | 38ms | ↓15% | <100ms | ✅ |
| Static | Medium | 92ms | 85ms | ↓8% | <100ms | ✅ |
| Embedded | Simple | 320ms | 280ms | ↓12% | <500ms | ✅ |
| Embedded | Medium | 480ms | 420ms | ↓12% | <500ms | ✅ |
| Embedded | Complex | 820ms | 750ms | ↓9% | <800ms | ✅ |

## Resource Usage

| Backend | Memory (Peak) | CPU (Active) | Change | Target | Status |
|---------|---------------|--------------|--------|--------|--------|
| Static | 42MB | 3% | - | <50MB | ✅ |
| Embedded | 180MB | 45% | ↓20MB | <250MB | ✅ |

## Key Improvements
- Optimized tokenization: 15% faster (caching improvements)
- Reduced model memory footprint: 20MB savings (quantization)
- Static matcher: 8% faster (regex optimization)

## Regressions
- None detected

## Next Steps
- Continue optimizing embedded backend (target: <400ms p95 for medium queries)
- Investigate tail latency (p99) spikes in load testing
```

---

## Optimization Priorities

### Priority Matrix

```
            HIGH IMPACT
                │
   P1 - QUICK WIN     P0 - CRITICAL
   (Do This Week)     (Do Today)
                │
───────────────┼───────────────
                │
   P3 - BACKLOG       P2 - PLAN
   (Do Someday)       (Do This Month)
                │
            LOW IMPACT
```

**Priority Definitions**:

**P0 - Critical** (Do Today)
- Performance blocker (app unusable, >2x target latency)
- Example: Embedded backend taking 5 seconds per query
- Action: Drop everything, fix immediately

**P1 - High** (Do This Week)
- Significant user impact (missing target by 20-50%)
- Example: Static matcher taking 150ms instead of 100ms
- Action: Prioritize in current sprint

**P2 - Medium** (Do This Month)
- Noticeable but not blocking (missing target by 10-20%)
- Example: Embedded backend taking 550ms instead of 500ms
- Action: Add to next sprint

**P3 - Low** (Do Someday)
- Minor improvement (<10% gain)
- Example: Reducing static matcher from 50ms to 45ms
- Action: Backlog, revisit when user-facing

---

### Current Optimization Priorities (v1.1.0-beta)

**Week 1 Priorities**:

1. **P0: Embedded Backend Cold Start** (Currently: 800ms, Target: <500ms)
   - **Issue**: First query after launch takes too long (model loading)
   - **Solution**: Lazy model loading, cache model in memory
   - **Impact**: 40% cold start improvement
   - **Effort**: 1 day
   - **Owner**: @alice

2. **P1: Static Matcher Coverage** (Currently: 68%, Target: 75%+)
   - **Issue**: Too many queries fall back to slower embedded backend
   - **Solution**: Add 50+ new patterns (file management, system monitoring)
   - **Impact**: 10% more queries use fast path (<100ms)
   - **Effort**: 2 days
   - **Owner**: @bob

3. **P2: Embedded Backend Tokenization** (Currently: 120ms, Target: <80ms)
   - **Issue**: Tokenization takes 25% of total inference time
   - **Solution**: Cache tokenizer, optimize hot loops
   - **Impact**: 15% overall latency reduction
   - **Effort**: 1 day
   - **Owner**: @alice

**Deferred to v1.2.0**:
- Quantization (INT8) for embedded models (30% speed improvement)
- MLX backend for Apple Silicon (2x faster than CPU inference)
- Async inference (non-blocking UI during generation)

---

## Backend-Specific Optimization

### Static Matcher Optimization

**Current Performance**: p95: 85ms (simple), 100ms (medium)

**Optimization Opportunities**:

1. **Regex Compilation Caching**
   - **Issue**: Compiling regex patterns on every query
   - **Solution**: Compile once at startup, cache in `HashMap`
   - **Impact**: 20% faster pattern matching

2. **Early Termination**
   - **Issue**: Checking all patterns even after finding a match
   - **Solution**: Return immediately on first match
   - **Impact**: 10% faster average case

3. **Pattern Ordering**
   - **Issue**: Common patterns checked last
   - **Solution**: Order patterns by frequency (most common first)
   - **Impact**: 15% faster average case

**Code Example** (Regex Caching):
```rust
// Before: Compile regex on every query
fn matches_pattern(&self, query: &str, pattern: &str) -> bool {
    let re = Regex::new(pattern).unwrap();  // ❌ Slow: Compile every time
    re.is_match(query)
}

// After: Cache compiled regex
pub struct StaticMatcher {
    patterns: HashMap<String, Regex>,  // ✅ Fast: Pre-compiled
}

impl StaticMatcher {
    pub fn new() -> Self {
        let mut patterns = HashMap::new();
        patterns.insert("list".to_string(), Regex::new(r"^list|show|display").unwrap());
        // ... more patterns
        StaticMatcher { patterns }
    }

    fn matches_pattern(&self, query: &str, pattern_name: &str) -> bool {
        self.patterns[pattern_name].is_match(query)  // ✅ Fast: Use cached regex
    }
}
```

---

### Embedded Backend Optimization

**Current Performance**: p95: 420ms (simple), 500ms (medium), 750ms (complex)

**Optimization Opportunities**:

1. **Model Quantization (INT8)**
   - **Issue**: FP32 models are 4x larger and slower than INT8
   - **Solution**: Quantize SmolLM-135M to INT8 (post-training quantization)
   - **Impact**: 30% faster inference, 75% smaller model (540MB → 135MB)
   - **Trade-off**: Slight accuracy loss (~2%), acceptable for CLI use case
   - **Effort**: 2 days (quantization + validation)
   - **Status**: Deferred to v1.2.0 (not critical for v1.1.0-beta)

2. **Batch Inference**
   - **Issue**: Processing one query at a time (underutilizing GPU/CPU)
   - **Solution**: Batch multiple queries (if user submits multiple)
   - **Impact**: 40% higher throughput (not applicable for single-user CLI)
   - **Status**: Deferred (low priority for CLI)

3. **KV-Cache Optimization**
   - **Issue**: Recomputing key-value cache for every token
   - **Solution**: Cache KV pairs for multi-turn conversations
   - **Impact**: 20% faster for multi-query sessions
   - **Status**: Deferred to v1.3.0 (requires session management)

4. **Prompt Compression**
   - **Issue**: Long system prompts increase token count
   - **Solution**: Compress prompts, remove redundant examples
   - **Impact**: 10% faster (fewer tokens to process)
   - **Effort**: 1 day (prompt engineering)
   - **Status**: **High Priority for v1.1.0-beta**

**Code Example** (Prompt Compression):
```rust
// Before: Verbose system prompt (250 tokens)
let system_prompt = r#"
You are a helpful assistant that converts natural language queries to shell commands.

Examples:
- User: "list all files in the current directory"
  Command: ls -la
- User: "find large files"
  Command: find . -type f -size +100M
// ... 10 more examples
"#;

// After: Compressed prompt (120 tokens)
let system_prompt = r#"
Convert queries to shell commands.

Examples:
list files → ls -la
find large files → find . -type f -size +100M
// ... 5 most relevant examples (pruned)
"#;
```

---

### Ollama Backend Optimization

**Current Performance**: p95: 950ms (simple), 1200ms (medium), 2000ms (complex)

**Optimization Opportunities**:

1. **Connection Pooling**
   - **Issue**: Opening new HTTP connection for every query
   - **Solution**: Reuse HTTP connection (persistent connection)
   - **Impact**: 50ms latency reduction (5% improvement)
   - **Effort**: 1 hour (use `reqwest` with keep-alive)

2. **Request Compression**
   - **Issue**: Large prompts over network (high latency)
   - **Solution**: Gzip compression for HTTP requests
   - **Impact**: 30% bandwidth reduction, 20ms latency reduction
   - **Effort**: 1 hour (enable gzip in `reqwest`)

3. **Streaming Responses**
   - **Issue**: Waiting for full response before displaying
   - **Solution**: Stream tokens as they arrive (progressive loading)
   - **Impact**: Perceived latency reduction (user sees results sooner)
   - **Effort**: 1 day (implement streaming API)
   - **Status**: **High Priority for v1.2.0**

**Code Example** (Connection Pooling):
```rust
// Before: New connection every query
fn generate(&self, query: &str) -> Result<String> {
    let client = reqwest::blocking::Client::new();  // ❌ Slow: New connection
    let response = client.post(self.ollama_url)
        .json(&request)
        .send()?;
    // ...
}

// After: Reuse connection
pub struct OllamaBackend {
    client: reqwest::blocking::Client,  // ✅ Fast: Persistent connection
}

impl OllamaBackend {
    pub fn new() -> Self {
        let client = reqwest::blocking::Client::builder()
            .tcp_keepalive(Some(Duration::from_secs(60)))
            .build()
            .unwrap();
        OllamaBackend { client }
    }

    fn generate(&self, query: &str) -> Result<String> {
        let response = self.client.post(&self.ollama_url)
            .json(&request)
            .send()?;
        // ...
    }
}
```

---

## Platform-Specific Optimization

### macOS Optimizations

**1. MLX Backend for Apple Silicon** (v1.2.0)
- **Goal**: Leverage Apple's Metal GPU for 2-3x faster inference
- **Benefit**: Native Apple Silicon optimization (M1/M2/M3)
- **Implementation**: MLX framework (Python → Rust bindings via PyO3 or native Rust bindings)
- **Effort**: 1 week (significant work)
- **Status**: Deferred to v1.2.0 (high impact but not critical for beta)

**2. macOS-Specific Compiler Flags**
- **Flags**: `-C target-cpu=native` (optimize for M1/M2/M3)
- **Benefit**: 5-10% performance improvement (CPU-specific instructions)
- **Status**: Already enabled in `Cargo.toml`

```toml
[profile.release]
opt-level = 3
lto = true
codegen-units = 1
target-cpu = "native"  # ✅ Optimize for user's CPU
```

---

### Linux Optimizations

**1. AVX2/AVX-512 Optimizations (x86_64)**
- **Goal**: Leverage SIMD instructions for faster math
- **Benefit**: 20-30% faster tokenization and inference
- **Implementation**: Rust SIMD (portable-simd feature)
- **Status**: Partially enabled (candle uses SIMD internally)

**2. ARM NEON Optimizations (ARM64)**
- **Goal**: Optimize for ARM servers (AWS Graviton, Raspberry Pi)
- **Benefit**: 15-25% faster inference on ARM
- **Implementation**: ARM NEON intrinsics
- **Status**: Partially enabled (candle supports ARM)

---

## Continuous Performance Monitoring

### Real-Time Monitoring

**Telemetry Metrics** (Collected in Production):
- Response time (p50, p95, p99) by backend
- Backend selection distribution (static vs. embedded vs. ollama)
- Error rates (timeouts, crashes)
- Resource usage (memory, CPU)

**Dashboard** (see `v1.1.0-release-metrics-dashboard-monitoring.md`):
```
┌──────────────────────────────────────────────────────────┐
│  PERFORMANCE (Last Hour)                                 │
├──────────────────────────────────────────────────────────┤
│  Response Time p95:  412ms [✓ <500ms]                   │
│  ├─ Static:          45ms                                │
│  ├─ Embedded:        380ms                               │
│  └─ Ollama:          920ms                               │
│                                                          │
│  Throughput:         12.3 cmd/min [✓ <100 cmd/min]      │
│  Backend Health:     ✓✓✓ All operational                │
└──────────────────────────────────────────────────────────┘
```

---

### Performance Alerts

**Alerting Rules** (see `v1.1.0-release-metrics-dashboard-monitoring.md`):

**P1 Alert: Performance Degradation**
- **Condition**: p95 latency >1000ms for >15 minutes
- **Action**: Investigate logs, identify bottleneck, consider rollback
- **Owner**: @engineering-lead

**P2 Alert: Throughput Drop**
- **Condition**: Throughput <5 qps for >30 minutes
- **Action**: Check system resources, investigate slow queries
- **Owner**: @on-call-engineer

---

## Performance Regression Prevention

### CI/CD Performance Gates

**Gate 1: Micro-Benchmark Regression**
- **Tool**: `cargo bench --baseline main`
- **Threshold**: >10% regression fails CI
- **Action**: Block PR until performance restored or justified

**Gate 2: Binary Size Regression**
- **Tool**: `stat -f %z target/release/caro` (macOS)
- **Threshold**: >5MB increase fails CI
- **Rationale**: Bloated binaries slow startup and distribution
- **Action**: Investigate dependency changes, remove unnecessary crates

**Example CI Configuration** (`.github/workflows/performance.yml`):
```yaml
name: Performance Tests

on: [pull_request]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run benchmarks
        run: |
          cargo bench --bench safety_validation -- --save-baseline pr
          cargo bench --bench pattern_matching -- --save-baseline pr
      - name: Compare to main
        run: |
          git checkout main
          cargo bench --bench safety_validation -- --save-baseline main
          cargo bench --bench pattern_matching -- --save-baseline main
          # Compare baselines (fail if >10% regression)
          cargo bench -- --baseline main --baseline-compare pr
```

---

### Pre-Release Performance Review

**Checklist** (Week 1, Day 6):
- [ ] Run full benchmark suite (all backends, all platforms)
- [ ] Compare to baseline (v1.0.0 or previous release)
- [ ] Verify all targets met (p95 latency, resource usage)
- [ ] Profile any regressions (identify root cause)
- [ ] Document improvements and trade-offs
- [ ] Update benchmark report (`.claude/releases/performance/`)

**Example Report Summary**:
```markdown
## v1.1.0-beta Performance Review

**Overall Assessment**: ✅ PASS (all targets met)

**Key Improvements**:
- Static matcher: 15% faster (regex caching)
- Embedded backend: 12% faster (tokenization optimization)
- Memory footprint: 20MB reduction (quantization)

**Regressions**: None

**Open Issues**: Tail latency (p99) spikes under load (investigate in v1.2.0)
```

---

## Document History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2026-01-08 | Engineering Lead | Initial performance optimization & benchmarking strategy |

---

**End of Document**
