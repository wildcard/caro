# Performance Optimization Strategy - v1.1.0

**Audience**: Engineering Team, Performance Engineers, QA Team
**Last Updated**: January 8, 2026

---

## Table of Contents

1. [Performance Vision](#performance-vision)
2. [Performance Requirements](#performance-requirements)
3. [Current Performance](#current-performance)
4. [Optimization Techniques](#optimization-techniques)
5. [Profiling & Measurement](#profiling--measurement)
6. [Bottleneck Analysis](#bottleneck-analysis)
7. [Performance Testing](#performance-testing)
8. [Monitoring & Alerts](#monitoring--alerts)
9. [Roadmap](#roadmap)
10. [Success Metrics](#success-metrics)

---

## Performance Vision

### Mission
**"Make Caro feel instant—faster than googling, faster than typing the command manually."**

### Performance Philosophy
1. **Perceived Speed > Actual Speed**: What users feel matters more than benchmark numbers
2. **Fast by Default**: Optimize common cases, not edge cases
3. **Predictable**: Consistent performance > occasionally blazing fast
4. **Efficient**: Minimize resource usage (CPU, memory, disk)
5. **Measurable**: Benchmark everything, track regressions

### User Experience Goals
- **Instant Feedback**: <100ms for acknowledgment (typing indicator, loading state)
- **Fast Results**: <1 second for 95% of queries
- **No Blocking**: Async operations, never freeze the UI
- **Lightweight**: <50 MB memory, <10 MB binary

---

## Performance Requirements

### Response Time Targets

| Query Type | Target (p95) | Current (v1.1.0) | Status |
|------------|--------------|------------------|--------|
| **Simple** (Static Matcher) | <50ms | 45ms | ✅ Met |
| **Medium** (LLM Generation) | <500ms | 450ms | ✅ Met |
| **Complex** (Multi-step Agent) | <1s | 850ms | ✅ Met |
| **Large Directory** (>10k files) | <3s | 2.1s | ✅ Met |

**Simple Query Examples**:
- "list files"
- "show current directory"
- "disk usage"

**Medium Query Examples**:
- "find files modified today"
- "show top 5 processes by memory"
- "search for TODO in all .rs files"

**Complex Query Examples**:
- "find large log files from last week and show their sizes"
- "list all running Docker containers with their memory usage"
- "show git commits from the last month by author"

---

### Resource Usage Targets

| Resource | Target | Current (v1.1.0) | Status |
|----------|--------|------------------|--------|
| **Peak Memory** | <50 MB | 38 MB | ✅ Met |
| **Binary Size** (stripped) | <10 MB | 8.2 MB | ✅ Met |
| **CPU Usage** (idle) | <1% | 0.2% | ✅ Met |
| **Startup Time** | <100ms | 85ms | ✅ Met |

---

### Throughput Targets

| Scenario | Target | Current (v1.1.0) | Status |
|----------|--------|------------------|--------|
| **Sequential Queries** | >100/min | 120/min | ✅ Met |
| **Concurrent Queries** | >10/sec | 15/sec | ✅ Met |

---

## Current Performance

### Baseline Benchmarks (v1.1.0-beta)

**Command Generation** (End-to-End):
```
Simple queries (static matcher):
  Mean: 45ms
  p50: 42ms
  p95: 58ms
  p99: 72ms

Medium queries (LLM fallback):
  Mean: 450ms
  p50: 420ms
  p95: 520ms
  p99: 680ms

Complex queries (multi-step):
  Mean: 850ms
  p50: 800ms
  p95: 980ms
  p99: 1200ms
```

**Component Breakdown** (Medium Query):
```
Total: 450ms
├─ Platform detection: 2ms (0.4%)
├─ Intent classification: 5ms (1.1%)
├─ Static matcher lookup: 8ms (1.8%)
├─ LLM generation: 380ms (84.4%)
├─ Safety validation: 35ms (7.8%)
├─ Platform adaptation: 15ms (3.3%)
└─ Output formatting: 5ms (1.1%)
```

**Key Insight**: 84% of time is LLM generation. This is our optimization target.

---

### Performance Strengths

**What's Fast**:
1. **Static Matcher**: 45ms average (instant feel)
2. **Platform Detection**: 2ms (cached after first call)
3. **Safety Validation**: 35ms (regex-based, very fast)
4. **Startup Time**: 85ms (fast CLI startup)

---

### Performance Weaknesses

**What's Slow**:
1. **LLM Generation**: 380ms (model inference)
2. **Large Directories**: 2.1s (file system traversal)
3. **Cold Start**: First query +100ms (model loading)

---

## Optimization Techniques

### 1. Static Matcher Expansion

**Goal**: Handle more queries without LLM (instant results)

**Current**: 4 static patterns (website examples)
**Target**: 50+ patterns (top queries)

**Approach**:
```rust
// Expand static patterns for common queries
static PATTERNS: &[StaticPattern] = &[
    StaticPattern {
        keywords: &["list", "files"],
        template: "ls -la",
        confidence: 0.95,
    },
    StaticPattern {
        keywords: &["files", "modified", "today"],
        template_fn: generate_find_modified_today,
        confidence: 0.90,
    },
    // ... 48 more patterns
];
```

**Expected Improvement**:
- 30% of queries → <50ms (vs 450ms)
- Savings: 400ms × 30% = 120ms average improvement

---

### 2. LLM Model Optimization

**Goal**: Faster inference without sacrificing accuracy

**Current**: SmolLM 135M (380ms)
**Options**:
1. **Quantization**: 8-bit or 4-bit models (2-4× faster)
2. **Smaller Models**: SmolLM 35M (faster, but less accurate)
3. **Model Caching**: Cache frequent queries
4. **Batch Inference**: Process multiple queries together (for concurrent use)

**Approach 1: Quantization** (Recommended)
```rust
// Load quantized model (Q8)
let model = Model::load_quantized("smollm-135m-q8.gguf")?;

// Expected: 380ms → 180ms (2.1× speedup)
// Accuracy loss: <2%
```

**Expected Improvement**:
- LLM queries: 380ms → 180ms
- Overall: 450ms → 250ms

---

### 3. Caching Strategy

**Goal**: Avoid redundant work for repeated queries

**Cache Layers**:
1. **Query Cache**: Exact query match → instant result
2. **Intent Cache**: Similar queries → same template
3. **Model Cache**: Embeddings, frequent patterns

**Implementation**:
```rust
use lru::LruCache;

struct QueryCache {
    cache: LruCache<String, GeneratedCommand>,
    max_size: usize,
}

impl QueryCache {
    fn get(&mut self, query: &str) -> Option<&GeneratedCommand> {
        self.cache.get(query)
    }

    fn insert(&mut self, query: String, command: GeneratedCommand) {
        self.cache.put(query, command);
    }
}

// Usage
if let Some(cached) = query_cache.get(query) {
    return Ok(cached.clone());  // Instant result
}
```

**Expected Improvement**:
- 20% of queries are repeats (user habits)
- Savings: 450ms → 0ms for cached queries
- Average improvement: 450ms × 20% = 90ms

---

### 4. Lazy Loading

**Goal**: Defer expensive initialization until needed

**Current**: Load all components on startup
**Target**: Lazy-load LLM model

**Implementation**:
```rust
struct Agent {
    static_matcher: StaticMatcher,
    llm_backend: OnceCell<LLMBackend>,  // Lazy-loaded
}

impl Agent {
    fn generate_command(&mut self, query: &str) -> Result<Command> {
        // Try static matcher first (no LLM loading)
        if let Some(cmd) = self.static_matcher.match_query(query) {
            return Ok(cmd);  // Fast path
        }

        // Load LLM only if needed
        let llm = self.llm_backend.get_or_init(|| LLMBackend::new());
        llm.generate(query)
    }
}
```

**Expected Improvement**:
- Cold start: 85ms → 10ms (75ms savings)
- First LLM query: +75ms one-time cost (amortized over session)

---

### 5. Parallel Processing

**Goal**: Overlap independent operations

**Opportunities**:
1. **Platform Detection + Intent Classification**: Run in parallel
2. **Safety Validation + Platform Adaptation**: Run in parallel
3. **Multiple Backend Queries**: Try multiple models, use fastest result

**Implementation**:
```rust
use tokio::join;

async fn generate_command(query: &str) -> Result<Command> {
    // Run platform detection and intent classification in parallel
    let (platform, intent) = join!(
        detect_platform(),
        classify_intent(query)
    );

    // Generate command
    let command = llm_generate(query, platform?, intent?).await?;

    // Run safety validation and platform adaptation in parallel
    let (safety_result, adapted_command) = join!(
        validate_safety(&command),
        adapt_platform(&command, platform?)
    );

    Ok(adapted_command?)
}
```

**Expected Improvement**:
- Overlap 20ms of work (platform detection + intent)
- Overall: 450ms → 430ms

---

### 6. Incremental Response

**Goal**: Show partial results while processing (perceived speed)

**Approach**:
```
User types query
  ↓
Instant acknowledgment (<100ms): "Generating command..."
  ↓
Static matcher check (45ms): If match, show immediately
  ↓
LLM generation (380ms): Stream partial results as generated
  ↓
Safety validation (35ms): Show final result
```

**User Experience**:
- Feels instant (acknowledgment within 100ms)
- Progressive disclosure (command appears character-by-character)
- Total time unchanged, but feels faster

---

## Profiling & Measurement

### Profiling Tools

**1. Cargo Flamegraph** (CPU profiling)
```bash
cargo install flamegraph
cargo flamegraph -- caro "complex query"

# Generates flamegraph.svg (CPU time by function)
```

**2. Perf** (Linux profiling)
```bash
perf record --call-graph dwarf cargo run -- caro "query"
perf report

# Shows hot paths, function call graphs
```

**3. Valgrind/Cachegrind** (Memory profiling)
```bash
valgrind --tool=cachegrind ./target/release/caro "query"

# Shows cache misses, memory allocations
```

**4. Criterion** (Benchmarking)
```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn benchmark_command_generation(c: &mut Criterion) {
    let agent = Agent::new(Config::default());

    c.bench_function("simple query", |b| {
        b.iter(|| agent.generate_command(black_box("list files")))
    });

    c.bench_function("medium query", |b| {
        b.iter(|| agent.generate_command(black_box("find files modified today")))
    });
}

criterion_group!(benches, benchmark_command_generation);
criterion_main!(benches);
```

---

### Measurement Best Practices

**1. Isolate Variables**:
- Benchmark on same hardware (consistent results)
- Warm cache vs cold cache (separate benchmarks)
- Control for background processes

**2. Statistical Rigor**:
- Run 100+ iterations (statistical significance)
- Report p50, p95, p99 (not just mean)
- Confidence intervals (is improvement real?)

**3. Real-World Scenarios**:
- Benchmark actual user queries (not synthetic)
- Test on target platforms (macOS, Linux)
- Include edge cases (large directories, complex queries)

---

## Bottleneck Analysis

### Identifying Bottlenecks

**Process**:
1. **Profile**: Run flamegraph or perf
2. **Analyze**: Identify functions consuming >10% of time
3. **Measure**: Quantify impact (how much time?)
4. **Optimize**: Implement fix
5. **Verify**: Re-profile, measure improvement

**Example Analysis** (Medium Query):
```
Total: 450ms

Top Bottlenecks:
1. LLM inference: 380ms (84.4%) ← PRIMARY TARGET
2. Safety validation (regex): 35ms (7.8%)
3. Platform adaptation (string manipulation): 15ms (3.3%)
4. Static matcher lookup (hash map): 8ms (1.8%)
5. Intent classification (string parsing): 5ms (1.1%)

Optimization Priority:
1. LLM inference (380ms → 180ms via quantization)
2. Safety validation (35ms → 20ms via compiled regex)
3. Platform adaptation (15ms → 10ms via template caching)
```

---

### Common Performance Anti-Patterns

**1. Premature Optimization**
- ❌ Optimize before measuring (wasted effort)
- ✅ Profile first, optimize bottlenecks

**2. Micro-Optimizations**
- ❌ Optimize 1ms function (negligible impact)
- ✅ Optimize 380ms function (significant impact)

**3. Algorithmic Complexity**
- ❌ O(n²) algorithm (scales poorly)
- ✅ O(n) or O(log n) (scales well)

**4. Excessive Allocations**
- ❌ Clone strings unnecessarily
- ✅ Use references, string slices

**5. Blocking I/O**
- ❌ Synchronous file reads (blocks thread)
- ✅ Async I/O (non-blocking)

---

## Performance Testing

### Benchmark Suite

**Unit Benchmarks** (Component-level):
```rust
#[bench]
fn bench_static_matcher(b: &mut Bencher) {
    let matcher = StaticMatcher::new();
    b.iter(|| matcher.match_query("list files"));
}

#[bench]
fn bench_safety_validation(b: &mut Bencher) {
    let validator = SafetyValidator::new();
    b.iter(|| validator.validate("rm -rf /"));
}
```

**Integration Benchmarks** (End-to-End):
```rust
#[bench]
fn bench_command_generation_simple(b: &mut Bencher) {
    let agent = Agent::new(Config::default());
    b.iter(|| agent.generate_command("list files"));
}

#[bench]
fn bench_command_generation_medium(b: &mut Bencher) {
    let agent = Agent::new(Config::default());
    b.iter(|| agent.generate_command("find files modified today"));
}
```

**Stress Tests** (Load Testing):
```bash
# Sequential load (100 queries)
for i in {1..100}; do
    time caro "query $i" > /dev/null
done

# Concurrent load (10 parallel, 10 iterations)
seq 1 10 | xargs -n1 -P10 sh -c 'caro "query" > /dev/null'
```

---

### Regression Testing

**Goal**: Detect performance regressions before release

**Process**:
1. **Baseline**: Establish performance baseline (v1.1.0)
2. **Continuous Benchmarking**: Run benchmarks on every commit (CI)
3. **Comparison**: Compare to baseline
4. **Alert**: If >10% slower → Block merge, investigate

**CI Integration** (.github/workflows/bench.yml):
```yaml
name: Performance Benchmarks

on: [push, pull_request]

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3
      - uses: dtolnay/rust-toolchain@stable

      - name: Run benchmarks
        run: cargo bench --bench command_generation

      - name: Compare to baseline
        run: |
          python scripts/compare-bench.py \
            --baseline benchmarks/v1.1.0-baseline.json \
            --current target/criterion/results.json \
            --threshold 0.10

      - name: Fail if regression
        if: failure()
        run: echo "Performance regression detected!" && exit 1
```

---

## Monitoring & Alerts

### Production Monitoring

**Metrics to Track** (Telemetry, opt-in):
1. **Response Time**: p50, p95, p99 (by query type)
2. **Throughput**: Queries per second
3. **Error Rate**: % of failed queries
4. **Cache Hit Rate**: % of queries served from cache

**Aggregation**:
- Per-user (identify slow queries)
- Per-platform (macOS vs Linux)
- Per-version (track regressions)

**Dashboard** (Grafana or similar):
```
┌─ Response Time (p95) ──────────────┐
│ Last 7 days                        │
│ [Line graph: 450ms → 420ms → 480ms]│
│ Current: 480ms (↑20ms from baseline)│
└────────────────────────────────────┘

┌─ Cache Hit Rate ───────────────────┐
│ Last 7 days                        │
│ [Line graph: 18% → 22% → 25%]     │
│ Current: 25% (↑7% from launch)    │
└────────────────────────────────────┘
```

---

### Performance Alerts

**Alert Conditions**:
1. **p95 Response Time** >1.5s (threshold: 1s target + 50% margin)
2. **Error Rate** >5% (threshold: should be <2%)
3. **Memory Usage** >75 MB (threshold: 50 MB target + 50% margin)

**Alert Actions**:
- Slack notification to #engineering
- Create GitHub issue (auto-labeled "performance")
- Escalate if P0 (system unusable)

---

## Roadmap

### v1.1.0 (Current - Jan 2026)
**Status**: ✅ Performance targets met

- [x] Response time <1s (p95): 980ms ✅
- [x] Memory <50 MB: 38 MB ✅
- [x] Binary <10 MB: 8.2 MB ✅
- [x] Static matcher: 4 patterns, 45ms

---

### v1.2.0 (Mar 2026)
**Focus**: Optimize LLM generation, expand static matcher

**Targets**:
- [ ] Response time <800ms (p95) ← 20% improvement
- [ ] Static matcher: 50+ patterns ← 10× expansion
- [ ] LLM quantization: 380ms → 180ms ← 2.1× speedup
- [ ] Query caching: 20% hit rate

**Implementation**:
1. **Expand Static Matcher** (Week 1-2):
   - Add 46 patterns (top queries from beta feedback)
   - Expected: 30% of queries <50ms

2. **LLM Quantization** (Week 3-4):
   - Test Q8 and Q4 quantized models
   - Benchmark accuracy vs speed trade-off
   - Choose best balance (likely Q8)

3. **Query Caching** (Week 5-6):
   - Implement LRU cache (1000 queries)
   - Track cache hit rate
   - Eviction policy: LRU, size limit

---

### v1.3.0 (Jun 2026)
**Focus**: Advanced optimizations, perceived speed

**Targets**:
- [ ] Response time <600ms (p95) ← 25% improvement
- [ ] Incremental response: Show acknowledgment <100ms
- [ ] Parallel processing: Overlap 50ms of work
- [ ] Large directory: 2.1s → 1.5s ← 30% improvement

**Implementation**:
1. **Incremental Response** (Week 1-2):
   - Stream LLM output character-by-character
   - Show "Generating..." within 100ms

2. **Parallel Processing** (Week 3-4):
   - Parallelize platform detection + intent classification
   - Parallelize safety validation + platform adaptation

3. **Large Directory Optimization** (Week 5-6):
   - Implement file traversal caching
   - Limit depth for find commands
   - Use fts(3) (BSD) or fts alternative (Linux)

---

### v1.4.0 (Sep 2026)
**Focus**: Edge case optimizations, mobile-friendly

**Targets**:
- [ ] Cold start: 85ms → 50ms
- [ ] Memory: 38 MB → 30 MB
- [ ] Binary: 8.2 MB → 5 MB (aggressive optimization)

---

## Success Metrics

### Performance KPIs

**Response Time** (Primary Metric):
- Target: <1s (p95)
- Stretch: <800ms (p95)
- Measurement: Telemetry (opt-in)

**User Satisfaction** (Proxy for Performance):
- Target: ≥4.0/5.0
- Question: "Caro feels fast and responsive"
- Measurement: User surveys

**Cache Efficiency**:
- Target: >20% hit rate (v1.2.0)
- Stretch: >30% hit rate (v1.3.0)
- Measurement: Telemetry

**Regression Rate**:
- Target: <5% releases have regressions
- Measurement: CI benchmarks, user reports

---

### Optimization Impact Tracking

**Before/After Comparison** (v1.1.0 → v1.2.0):
```
Simple Queries (Static Matcher):
  Before: 45ms (4 patterns)
  After:  40ms (50+ patterns)
  Improvement: 11% faster, 10× more coverage

Medium Queries (LLM):
  Before: 450ms
  After:  250ms (quantization + caching)
  Improvement: 44% faster

Complex Queries:
  Before: 850ms
  After:  600ms
  Improvement: 29% faster

Overall (Weighted Average):
  Before: 450ms
  After:  250ms
  Improvement: 44% faster
```

---

## Summary

### Performance Vision
Make Caro feel instant—faster than googling, faster than typing the command manually.

### Current Performance (v1.1.0)
- Simple: 45ms, Medium: 450ms, Complex: 850ms
- All targets met (<1s p95) ✅
- Primary bottleneck: LLM inference (84% of time)

### Optimization Techniques (6 Key)
1. Static matcher expansion (4 → 50+ patterns)
2. LLM quantization (380ms → 180ms)
3. Query caching (20% hit rate, instant for repeats)
4. Lazy loading (cold start 85ms → 10ms)
5. Parallel processing (overlap 20ms)
6. Incremental response (perceived speed)

### Roadmap
- v1.2.0 (Mar 2026): 450ms → 250ms (44% faster)
- v1.3.0 (Jun 2026): 250ms → 150ms (40% faster)
- v1.4.0 (Sep 2026): Edge case optimizations

### Success Metrics
- Response time <1s p95 (current: 980ms ✅)
- User satisfaction ≥4.0/5.0
- Cache hit rate >20%
- Regression rate <5%

---

**Document Version**: 1.0
**Last Updated**: January 8, 2026
**Owner**: Engineering Team, Performance Engineers
