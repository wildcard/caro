# v1.1.0 Testing Strategy & QA Guide

**Purpose**: Comprehensive testing approach for quality assurance across all release phases
**Audience**: QA Team, Engineers, Beta Testers, Release Manager
**Scope**: Pre-beta, beta testing, GA validation, post-release monitoring

---

## Testing Philosophy

### Core Principles

1. **Privacy First**: Every test validates ZERO PII collection
2. **Safety Critical**: Command safety is non-negotiable
3. **User-Centric**: Test real-world user scenarios
4. **Platform Diversity**: Test across macOS, Linux, and edge cases
5. **Automated + Manual**: Combine automation with human judgment

### Quality Standards

**Acceptance Criteria**:
- Pass Rate: ≥75% for static matcher
- P0 Bugs: 0 (blocking bugs)
- P1 Bugs: ≤3 (high priority bugs)
- Privacy: ZERO PII collected (absolute requirement)
- Installation Success: ≥80%
- User Satisfaction: ≥4.0/5.0

---

## Testing Pyramid

```
        /\
       /  \  Manual Exploratory (10%)
      /____\
     /      \
    /        \ Integration Tests (20%)
   /__________\
  /            \
 /              \ Unit Tests (70%)
/________________\
```

### Layer 1: Unit Tests (70% of test effort)

**Purpose**: Test individual functions and components in isolation

**Tools**:
- `cargo test` (Rust unit tests)
- `cargo test --doc` (documentation tests)

**Coverage Target**: 80%+ code coverage

**Examples**:
```rust
#[test]
fn test_static_matcher_finds_pattern() {
    let matcher = StaticMatcher::new();
    let result = matcher.match_pattern("list files modified today");
    assert_eq!(result, Some("find . -type f -mtime 0"));
}

#[test]
fn test_privacy_validator_blocks_pii() {
    let validator = PrivacyValidator::new();
    let result = validator.validate("user@email.com");
    assert!(result.is_err()); // Should reject PII
}
```

**Run Frequency**: Every commit (CI/CD)

---

### Layer 2: Integration Tests (20% of test effort)

**Purpose**: Test component interactions and end-to-end flows

**Tools**:
- Integration test suite in `tests/`
- `cargo test --test integration`

**Focus Areas**:
- Backend integration (static ↔ embedded)
- Telemetry collection and upload
- Command generation pipeline
- Safety validation pipeline

**Examples**:
```rust
#[test]
fn test_command_generation_with_fallback() {
    // Test: Static fails → Embedded fallback
    let agent = AgentLoop::new();
    let result = agent.generate("complex query beyond static");
    assert!(result.is_ok());
    assert_eq!(result.backend_used, BackendType::Embedded);
}

#[test]
fn test_telemetry_e2e() {
    // Test: Event collection → Storage → Upload
    telemetry::record_event(Event::SessionStart);
    let stored = telemetry::get_events();
    assert_eq!(stored.len(), 1);
    // Verify ZERO PII
    assert!(!contains_pii(&stored[0]));
}
```

**Run Frequency**: Every PR, before release

---

### Layer 3: Manual Exploratory (10% of test effort)

**Purpose**: Human judgment, creativity, edge cases

**When**: Before each release, during beta testing

**Focus**:
- Usability issues
- Unexpected behaviors
- Platform-specific quirks
- Real-world workflows

**Approach**: Scenario-based testing (see Beta Testing section)

---

## Pre-Beta Testing (Cycles 0-10)

### Cycle 0: Baseline Testing

**Goal**: Establish current quality baseline

**Test Suite**: 58 test cases across 8 categories

**Execution**:
```bash
# Run all test cases with bt_001 profile
cd .claude/beta-testing
./run_tests.sh --profile bt_001 --output cycle-0-results.json

# Analyze results
python analyze_results.py cycle-0-results.json
```

**Output**: Baseline pass rate by category

**Example Results**:
```
Category             | Pass Rate | Notes
---------------------|-----------|------------------
Website Claims (P0)  | 100%      | All 4 examples pass
File Management      | 30%       | Static matcher gaps
System Monitoring    | 20%       | Needs LLM fallback
DevOps/Kubernetes    | 10%       | Complex queries
Text Processing      | 20%       | Regex patterns needed
Network Diagnostics  | 25%       | Platform-specific
Git Operations       | 40%       | Partial coverage
Overall              | 30%       | Below 75% target
```

---

### Cycles 1-9: Iterative Improvement

**Goal**: Achieve ≥75% pass rate through systematic improvements

**Each Cycle**:
1. **Identify**: Lowest performing category
2. **Fix**: Add static patterns or improve prompts
3. **Test**: Re-run full test suite
4. **Measure**: Document improvement
5. **Repeat**: Until target reached

**Example Cycle 1** (File Management):
```bash
# Before: 30% pass rate
# Action: Add 15 new static patterns
# After: 70% pass rate (+40%)
```

**Tracking**: Document in `.claude/beta-testing/cycles/cycle-N-*.md`

---

### Cycle 10: Prompt Engineering

**Goal**: Optimize embedded backend for quality and consistency

**Changes**:
- Chain-of-thought prompting (5-step reasoning)
- Platform-specific negative examples
- Expanded few-shot examples (8 → 25+)
- Temperature optimization (0.7 → 0.1)

**Testing**:
```bash
# Test embedded backend specifically
cargo test --test embedded_backend -- --nocapture

# Run full suite
./run_tests.sh --profile bt_002 --backend embedded
```

**Expected**: Improved LLM fallback quality

---

## Beta Testing Strategy

### Phase 1: Preparation (Jan 8-12)

**Pre-Flight Checklist** (Jan 12):

#### Build & Installation Testing
- [ ] Clean build succeeds on all platforms
  ```bash
  cargo clean && cargo build --release
  ```
- [ ] Binaries created: macOS (x86_64, aarch64), Linux (x86_64)
- [ ] Installation via cargo works:
  ```bash
  cargo install caro --version 1.1.0-beta
  ```
- [ ] Installation via script works:
  ```bash
  curl -sSL https://caro.sh/install.sh | sh
  ```

#### Core Functionality Testing
- [ ] Version command works:
  ```bash
  caro --version  # Should show 1.1.0-beta
  ```
- [ ] Help command works:
  ```bash
  caro --help
  ```
- [ ] Test 10 representative commands:
  - Website examples (4 commands) - 100% pass required
  - File operations (3 commands)
  - System monitoring (2 commands)
  - Text processing (1 command)

#### Privacy & Security Testing (CRITICAL)
- [ ] **Manual privacy audit** (30 minutes)
  - Generate 20 diverse commands
  - Inspect ALL telemetry events
  - Verify: ZERO PII (emails, paths, usernames, IP addresses)
  - **BLOCKER**: Any PII found = NO-GO

- [ ] Run privacy test suite:
  ```bash
  cargo test privacy -- --nocapture
  # Must show: All tests passed, 220+ tests
  ```

- [ ] Test consent flow:
  ```bash
  # First run should prompt
  caro "test command"
  # Should show consent prompt and explain telemetry
  ```

- [ ] Test opt-out:
  ```bash
  caro telemetry disable
  # Generate commands, verify NO telemetry collected
  ```

#### Safety Validation Testing
- [ ] Test safety patterns:
  ```bash
  # Should block or warn
  caro "delete everything"
  caro "format disk"
  caro "rm -rf /"
  ```

- [ ] Test safe commands (should allow):
  ```bash
  caro "list files"
  caro "show disk usage"
  caro "find large files"
  ```

---

### Phase 2: Active Beta (Jan 13-17)

**Daily Testing Activities**:

#### Day 1 (Jan 13): Installation & Onboarding
**Focus**: First-run experience

**Test Cases**:
- [ ] Fresh install on clean system
- [ ] First command generation
- [ ] Help system usability
- [ ] Error message clarity

**Success Metrics**:
- Installation success: 100% (3/3 testers)
- First command generated: < 2 minutes
- Consent flow understood: 100%

---

#### Day 2 (Jan 14): Core Functionality
**Focus**: Command generation quality

**Test Cases**:
- [ ] Static matcher commands (website examples)
- [ ] File management queries
- [ ] System monitoring commands
- [ ] Text processing queries

**Test Matrix**: 20 commands per tester (60 total)

**Success Metrics**:
- Overall pass rate: ≥75%
- Static matcher: 100% (website examples)
- Embedded backend: ≥60%

---

#### Day 3 (Jan 15): Edge Cases & Platforms
**Focus**: Robustness and platform compatibility

**Test Cases**:
- [ ] Long/complex queries
- [ ] Ambiguous queries
- [ ] Non-English queries (if supported)
- [ ] Platform-specific commands (macOS vs Linux)
- [ ] Edge cases (empty input, special characters)

**Platform Matrix**:
- macOS Intel (bt_001)
- macOS Apple Silicon (bt_003)
- Linux Ubuntu (bt_002)
- Linux Arch/Fedora (bt_004, bt_005)

**Success Metrics**:
- No crashes: 100%
- Platform-specific commands work correctly: ≥80%
- Graceful error handling: 100%

---

#### Day 4 (Jan 16): Telemetry & Privacy
**Focus**: Privacy validation and telemetry quality

**Test Cases**:
- [ ] Generate 50+ commands per tester
- [ ] Export telemetry data:
  ```bash
  caro telemetry export --output telemetry.json
  ```
- [ ] Manual inspection of exported data
- [ ] Verify: ZERO PII in all events
- [ ] Test telemetry commands:
  ```bash
  caro telemetry show
  caro telemetry clear
  caro telemetry status
  ```

**Success Metrics**:
- ZERO PII found: 100% (BLOCKER if violated)
- Telemetry data quality: Complete event metadata
- Commands work correctly: 100%

---

#### Day 5 (Jan 17): Real-World Workflows
**Focus**: Practical use cases

**Test Scenarios**:
- **DevOps Engineer**: Kubernetes debugging, log analysis
- **SRE**: System monitoring, incident response
- **Developer**: Git operations, file management
- **Student**: Learning command line, exploring Linux
- **Data Analyst**: Text processing, CSV manipulation

**Success Metrics**:
- Workflow completion: ≥80%
- User satisfaction per scenario: ≥4.0/5.0
- Friction points documented

---

### Phase 3: Analysis (Jan 18-19)

**Quantitative Analysis**:

```python
# analyze_beta_results.py

def analyze_pass_rate(test_results):
    total = len(test_results)
    passed = sum(1 for r in test_results if r.status == "pass")
    return passed / total * 100

def analyze_by_category(test_results):
    categories = {}
    for result in test_results:
        cat = result.category
        if cat not in categories:
            categories[cat] = {"total": 0, "passed": 0}
        categories[cat]["total"] += 1
        if result.status == "pass":
            categories[cat]["passed"] += 1

    for cat, data in categories.items():
        pass_rate = data["passed"] / data["total"] * 100
        print(f"{cat}: {pass_rate:.1f}%")

def analyze_privacy(telemetry_data):
    pii_patterns = [
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
        r'/home/\w+',  # Home directory paths
        r'/Users/\w+',  # macOS user paths
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}',  # IP addresses
    ]

    violations = []
    for event in telemetry_data:
        event_str = json.dumps(event)
        for pattern in pii_patterns:
            matches = re.findall(pattern, event_str)
            if matches:
                violations.append({
                    "event": event,
                    "pattern": pattern,
                    "matches": matches
                })

    return violations  # MUST be empty list

# Run analysis
results = load_test_results("beta-results.json")
pass_rate = analyze_pass_rate(results)
print(f"Overall Pass Rate: {pass_rate}%")

analyze_by_category(results)

telemetry = load_telemetry_data("telemetry-exports/*.json")
privacy_violations = analyze_privacy(telemetry)
assert len(privacy_violations) == 0, "CRITICAL: PII found in telemetry!"
```

**Qualitative Analysis**:
- Review survey feedback (12 sections)
- Identify common themes
- Extract user quotes
- Document friction points

**Output**: Comprehensive analysis report in `.claude/beta-testing/analysis/`

---

## Test Case Categories

### Category 1: Website Claims (P0 - CRITICAL)

**Why P0**: These are prominently displayed examples on website/README

**Test Cases** (4 total):
1. "list all files modified today"
   - Expected: `find . -type f -mtime 0`
   - Priority: P0
   - Must Pass: YES

2. "find large files over 100MB"
   - Expected: `find . -type f -size +100M`
   - Priority: P0
   - Must Pass: YES

3. "show disk usage by folder"
   - Expected: `du -sh */ | sort -rh | head -10`
   - Priority: P0
   - Must Pass: YES

4. "find python files modified last week"
   - Expected: `find . -name "*.py" -type f -mtime -7`
   - Priority: P0
   - Must Pass: YES

**Success Criteria**: 100% pass rate (4/4)

---

### Category 2: File Management (15 test cases)

**Examples**:
- "files modified yesterday"
- "files larger than 50MB"
- "list files by extension"
- "find empty directories"
- "show hidden files"
- "files owned by user"
- "recently accessed files"
- "duplicate files by name"
- "files modified in last hour"
- "largest files in directory"

**Success Criteria**: ≥70% pass rate (11+/15)

---

### Category 3: System Monitoring (12 test cases)

**Examples**:
- "show running processes"
- "top 10 memory consuming processes"
- "check disk space"
- "show CPU usage"
- "list active network connections"
- "system uptime"
- "logged in users"
- "recent system logs"
- "check load average"
- "monitor specific process"

**Success Criteria**: ≥60% pass rate (8+/12)

---

### Category 4: Text Processing (10 test cases)

**Examples**:
- "search for text in files"
- "count lines in file"
- "find and replace text"
- "extract specific columns from CSV"
- "remove duplicate lines"
- "sort lines alphabetically"
- "filter lines containing pattern"
- "count word occurrences"
- "convert file encoding"
- "merge multiple files"

**Success Criteria**: ≥60% pass rate (6+/10)

---

### Category 5: Network Diagnostics (8 test cases)

**Examples**:
- "check if port is open"
- "find process using port 8080"
- "test network connectivity"
- "show network interfaces"
- "check DNS resolution"
- "measure network bandwidth"
- "trace route to host"
- "show open connections"

**Success Criteria**: ≥60% pass rate (5+/8)

---

### Category 6: Git Operations (8 test cases)

**Examples**:
- "show recent commits"
- "find commits by author"
- "show changed files"
- "list branches"
- "show commit history"
- "find commits with keyword"
- "show file history"
- "compare branches"

**Success Criteria**: ≥70% pass rate (6+/8)

---

### Category 7: DevOps/Kubernetes (5 test cases)

**Examples**:
- "list kubernetes pods"
- "check pod logs"
- "show docker containers"
- "get pod status"
- "view service endpoints"

**Success Criteria**: ≥50% pass rate (3+/5)

---

### Category 8: Edge Cases (10 test cases)

**Examples**:
- Empty input
- Very long query (>500 chars)
- Special characters
- Non-English characters
- Ambiguous queries
- Contradictory requirements
- Impossible requests
- Malformed queries
- Platform-incompatible requests
- Safety-violating requests

**Success Criteria**: Graceful handling (no crashes), appropriate errors

---

## Automated Testing

### CI/CD Pipeline

**GitHub Actions**: `.github/workflows/test.yml`

```yaml
name: Test Suite

on: [push, pull_request]

jobs:
  test:
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
        rust: [stable]

    runs-on: ${{ matrix.os }}

    steps:
      - uses: actions/checkout@v3

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ matrix.rust }}

      - name: Run unit tests
        run: cargo test --lib

      - name: Run integration tests
        run: cargo test --test '*'

      - name: Run privacy tests
        run: cargo test privacy -- --nocapture

      - name: Check code coverage
        run: |
          cargo install cargo-tarpaulin
          cargo tarpaulin --out Xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

**Run Frequency**: Every push, every PR

---

### Nightly Testing

**Schedule**: Every night at 2 AM UTC

**Purpose**: Catch regressions early

**Tests**:
- Full unit test suite
- Integration tests
- Static matcher validation (58 test cases)
- Privacy audit (automated)
- Performance benchmarks

**Notification**: Slack/email if failures detected

---

## Performance Testing

### Benchmarks

**Metrics**:
1. **Command Generation Latency**
   - Static matcher: < 10ms (target: < 5ms)
   - Embedded backend: < 500ms (target: < 300ms)

2. **Telemetry Overhead**
   - Per-event cost: < 5ms (target: < 1ms)
   - **Achieved**: 0.002ms (2500x better than target!)

3. **Memory Usage**
   - Baseline: < 50MB
   - With model loaded: < 500MB

4. **Binary Size**
   - Release binary: < 20MB (target)
   - Minimal dependencies

**Benchmark Suite**:
```rust
// benches/command_generation.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn bench_static_matcher(c: &mut Criterion) {
    let matcher = StaticMatcher::new();
    c.bench_function("static_matcher", |b| {
        b.iter(|| {
            matcher.generate_command(black_box("list files modified today"))
        })
    });
}

fn bench_embedded_backend(c: &mut Criterion) {
    let backend = EmbeddedBackend::new();
    c.bench_function("embedded_backend", |b| {
        b.iter(|| {
            backend.generate_command(black_box("complex query"))
        })
    });
}

criterion_group!(benches, bench_static_matcher, bench_embedded_backend);
criterion_main!(benches);
```

**Run**:
```bash
cargo bench
```

---

## Platform-Specific Testing

### macOS Testing

**Platforms**:
- macOS Intel (x86_64)
- macOS Apple Silicon (aarch64)

**Focus Areas**:
- BSD command variants
- `ps aux` without GNU flags
- `find` without `-printf`
- `stat -f` (BSD format)
- Homebrew integration

**Test Commands**:
```bash
# BSD-specific
caro "show process memory usage"  # Should use BSD ps
caro "find files with details"    # Should use BSD find
caro "file metadata"               # Should use stat -f
```

---

### Linux Testing

**Distributions**:
- Ubuntu/Debian (apt)
- Fedora/RHEL (dnf/yum)
- Arch (pacman)

**Focus Areas**:
- GNU command flags
- `ps aux --sort`
- `find -printf`
- `stat -c` (GNU format)
- Package manager commands

**Test Commands**:
```bash
# GNU-specific
caro "show processes sorted by memory"  # Should use GNU ps
caro "find files with formatted output" # Should use GNU find
caro "file metadata"                    # Should use stat -c
```

---

## Regression Testing

### Purpose

Ensure new changes don't break existing functionality

### Approach

**After each fix**:
1. Add regression test
2. Verify bug is fixed
3. Add to CI/CD pipeline
4. Document in test suite

**Example**:
```rust
// Issue #123: Static matcher returns wrong command for "files modified yesterday"
#[test]
fn test_issue_123_regression() {
    let matcher = StaticMatcher::new();
    let result = matcher.match_pattern("files modified yesterday");
    // Bug was: returned "find . -mtime 1" (wrong)
    // Fix: should be "find . -type f -mtime 1"
    assert_eq!(result, Some("find . -type f -mtime 1"));
}
```

---

## Load Testing

### Telemetry Backend

**Scenario**: High volume of telemetry uploads

**Test**:
```bash
# Simulate 1000 concurrent uploads
ab -n 1000 -c 100 -p telemetry_event.json \
   -T application/json \
   https://telemetry.caro.sh/events
```

**Success Criteria**:
- 99% requests succeed
- P99 latency < 500ms
- No data loss

**Note**: Current v1.1.0 has no telemetry backend yet (future)

---

## Security Testing

### Vulnerability Scanning

**Tools**:
```bash
# Dependency vulnerabilities
cargo audit

# Security issues in code
cargo clippy -- -W clippy::all

# Outdated dependencies
cargo outdated
```

**Run Frequency**: Weekly, before release

---

### Privacy Testing (CRITICAL)

**Manual Privacy Audit**:
1. Generate 50+ diverse commands
2. Export telemetry:
   ```bash
   caro telemetry export --output audit.json
   ```
3. **Manually inspect EVERY event**
4. Check for PII:
   - Emails
   - File paths (especially `/home/`, `/Users/`)
   - IP addresses
   - Usernames
   - API keys
   - Environment variables
   - Command arguments containing sensitive data

**Automated Privacy Tests**:
```rust
#[test]
fn test_privacy_no_email_addresses() {
    let events = generate_test_events(100);
    let email_pattern = Regex::new(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b").unwrap();

    for event in events {
        let json = serde_json::to_string(&event).unwrap();
        assert!(!email_pattern.is_match(&json),
                "Found email in event: {}", json);
    }
}

#[test]
fn test_privacy_no_file_paths() {
    let events = generate_test_events(100);
    let path_patterns = vec![
        Regex::new(r"/home/\w+").unwrap(),
        Regex::new(r"/Users/\w+").unwrap(),
        Regex::new(r"C:\\Users\\\w+").unwrap(),
    ];

    for event in events {
        let json = serde_json::to_string(&event).unwrap();
        for pattern in &path_patterns {
            assert!(!pattern.is_match(&json),
                    "Found file path in event: {}", json);
        }
    }
}
```

**BLOCKER**: Any PII found = NO RELEASE

---

## Test Reporting

### Daily Test Report (During Beta)

**Template**:
```markdown
# Beta Testing Daily Report - Day N

**Date**: YYYY-MM-DD
**Testers Active**: X/Y

## Test Execution
- Commands Generated: XXX
- Test Cases Run: XXX
- Pass Rate: XX%

## Issues Found
- P0 Bugs: X (list)
- P1 Bugs: X (list)
- P2 Bugs: X (list)

## Privacy Audit
- Events Collected: XXX
- PII Found: 0 ✅ (or BLOCKER if > 0)

## User Feedback
- Positive: [quotes]
- Issues: [quotes]
- Suggestions: [quotes]

## Blockers
- [list any blockers]

## Tomorrow's Plan
- [planned testing activities]
```

---

### Final Test Report (Jan 19)

**Comprehensive Summary**:
```markdown
# v1.1.0-beta Final Test Report

**Testing Period**: Jan 13-17, 2026
**Testers**: 3-5 beta testers

## Summary
- Overall Pass Rate: XX%
- Website Examples: 100% ✅
- Installation Success: XX%
- User Satisfaction: X.X/5.0

## Detailed Results
[Full breakdown by category]

## Privacy Validation
- Total Events Collected: XXX
- PII Found: 0 ✅
- Manual Audit: PASSED ✅

## Bug Summary
- P0 Bugs Found: X
- P0 Bugs Fixed: X
- P0 Bugs Open: 0 (MUST be 0 for GA)

## Recommendation
**GO / NO-GO** for GA release
```

---

## Test Artifacts

### Required Documentation

- [ ] **Test Plan**: This document
- [ ] **Test Cases**: `.claude/beta-testing/test-cases.yaml`
- [ ] **Test Results**: `.claude/beta-testing/cycles/cycle-N-results.json`
- [ ] **Privacy Audit Report**: `.claude/beta-testing/privacy-audit-report.md`
- [ ] **Bug Reports**: GitHub issues with `bug` label
- [ ] **Final Test Report**: `.claude/beta-testing/analysis/final-report.md`

---

## Success Criteria Summary

### Pre-Beta (Cycles 0-10)
- ✅ Overall pass rate: ≥75% (Achieved: 86.2%)
- ✅ Website examples: 100% (Achieved: 100%)
- ✅ Privacy: ZERO PII (Verified: ZERO)

### Beta Testing
- ⏸️ Installation success: ≥80%
- ⏸️ User satisfaction: ≥4.0/5.0
- ⏸️ P0 bugs: 0
- ⏸️ Privacy: ZERO PII (ongoing validation)

### GA Release
- ⏸️ All beta criteria met
- ⏸️ 30-day stability period
- ⏸️ Community feedback positive
- ⏸️ No critical issues

---

**Document Status**: Complete and ready for use
**Next Review**: After beta testing (update with actual results)
**Maintenance**: Update after each testing cycle with lessons learned
