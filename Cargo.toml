[package]
name = "caro"
version = "1.0.3"
edition = "2021"
description = "Convert natural language to shell commands using local LLMs"
license = "AGPL-3.0"
repository = "https://github.com/wildcard/caro"
readme = "README.crates.io.md"
authors = ["Claude Code <noreply@anthropic.com>"]
keywords = ["cli", "llm", "shell", "commands", "ai"]
categories = ["command-line-utilities", "development-tools"]

# Binary target
[[bin]]
name = "caro"
path = "src/main.rs"

[dependencies]
# Core CLI and async runtime
clap = { version = "4.5", features = ["derive", "env", "color"] }
tokio = { version = "1", features = ["rt-multi-thread", "macros", "time", "sync", "fs"] }

# Serialization and configuration
serde = { version = "1", features = ["derive"] }
serde_json = "1"
serde_yaml = "0.9"
toml = "0.9"
config = "0.15"

# HTTP client for remote backends (optional - feature-gated)
reqwest = { version = "0.12", features = ["json", "rustls-tls"], default-features = false, optional = true }

# Error handling and logging
anyhow = "1"
thiserror = "2"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt"] }
tracing-appender = "0.2"

# User interface and interaction
dialoguer = "0.12"
colored = "3"
indicatif = "0.18"

# Cross-platform support
directories = "6"
dirs = "6"
which = "8.0"
os_info = "3.14"
sysinfo = "0.37"

# Model integration (Hugging Face Hub)
hf-hub = { version = "0.4", default-features = false, features = ["tokio", "rustls-tls"] }
url = "2.5"

# Date/time handling
chrono = { version = "0.4", features = ["serde"] }

# Regex for safety validation
regex = "1"
once_cell = "1.19"

# Cryptography for checksums
sha2 = "0.10"

# Async trait support
async-trait = "0.1"
futures = "0.3"

# Optional MLX support for Apple Silicon (behind feature flag)
cxx = { version = "1.0", optional = true }

# Embedded model inference - MLX for Apple Silicon via llama.cpp
[target.'cfg(all(target_os = "macos", target_arch = "aarch64"))'.dependencies]
mlx-rs = { version = "0.25", optional = true }
llama_cpp = { version = "0.3", features = ["metal"], optional = true }

# Embedded model inference - Candle for CPU (cross-platform)
candle-core = { version = "0.9", optional = true }
candle-transformers = { version = "0.9", optional = true }
tokenizers = { version = "0.22", default-features = false, features = ["http", "rustls-tls", "onig"] }

[dev-dependencies]
tokio-test = "0.4"
tempfile = "3"
serial_test = "3"
proptest = "1"
criterion = { version = "0.8", features = ["html_reports"] }
futures = "0.3"

[[bench]]
name = "performance"
harness = false

[features]
default = ["embedded-mlx", "embedded-cpu"]  # Always build with MLX on macOS
mock-backend = []
remote-backends = ["reqwest", "tokio/net"]
embedded-mlx = ["cxx", "llama_cpp"]
embedded-cpu = ["candle-core", "candle-transformers"]
embedded = ["embedded-cpu"]  # Alias for embedded model support
full = ["remote-backends", "embedded-mlx", "embedded-cpu"]

[profile.release]
# Optimize for binary size (<50MB target)
opt-level = "z"     # Optimize for size
lto = true          # Link-time optimization
strip = true        # Strip debug symbols
codegen-units = 1   # Single codegen unit for better optimization
panic = "abort"     # Smaller binary size

[profile.dev]
# Fast compilation for development
opt-level = 0
debug = true
overflow-checks = true

# Benchmark profile
[profile.bench]
opt-level = 3
debug = false
lto = true

# Profile for release with debug info (useful for profiling)
[profile.release-with-debug]
inherits = "release"
debug = true
strip = false