{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FunctionGemma CLI Tool Recommender\n",
        "\n",
        "Fine-tune FunctionGemma (270M) to recommend CLI tools based on:\n",
        "- User queries\n",
        "- Operating system (macOS, Linux, Ubuntu, Windows, etc.)\n",
        "- Shell type (bash, zsh, fish, etc.)\n",
        "- User preferences\n",
        "\n",
        "**Based on official Unsloth FunctionGemma notebook with:**\n",
        "- `tokenizer.apply_chat_template()` for proper formatting\n",
        "- `train_on_responses_only()` for better training\n",
        "- Higher LoRA rank (r=128) for quality\n",
        "- HF-style tool schemas\n",
        "\n",
        "**Requirements:** Free Colab T4 GPU"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.3\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Model and Tokenizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 4096\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/functiongemma-270m-it\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=False,\n",
        "    load_in_8bit=False,\n",
        "    load_in_16bit=True,\n",
        "    full_finetuning=False,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model.config._name_or_path}\")\n",
        "print(f\"Parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Add LoRA Adapters (Higher Rank for Quality)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=128,  # Higher rank for better quality\n",
        "    lora_alpha=256,  # 2x rank\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters applied (r=128, alpha=256)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Tool Schema (HF-style)\n",
        "\n",
        "This is the proper way to define tools for FunctionGemma."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# CLI Tool Recommendation function schema (HF-style)\n",
        "CLI_TOOL_SCHEMA = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"recommend_tools\",\n",
        "        \"description\": \"Recommend CLI tools for a task based on OS, shell, and preferences.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"primary_tools\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"description\": \"Tools most likely installed by default\",\n",
        "                    \"items\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"name\": {\"type\": \"string\"},\n",
        "                            \"category\": {\"type\": \"string\"},\n",
        "                            \"confidence\": {\"type\": \"number\"},\n",
        "                            \"reason\": {\"type\": \"string\"},\n",
        "                        },\n",
        "                        \"required\": [\"name\", \"category\", \"confidence\", \"reason\"]\n",
        "                    }\n",
        "                },\n",
        "                \"alternative_tools\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"description\": \"Modern alternatives that may need installation\",\n",
        "                    \"items\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"name\": {\"type\": \"string\"},\n",
        "                            \"install_cmd\": {\"type\": \"string\"},\n",
        "                            \"reason\": {\"type\": \"string\"},\n",
        "                            \"improvements\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                        },\n",
        "                        \"required\": [\"name\", \"install_cmd\", \"reason\"]\n",
        "                    }\n",
        "                },\n",
        "                \"task_category\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\"primary_tools\", \"task_category\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "tools = [CLI_TOOL_SCHEMA]\n",
        "print(\"Tool schema defined\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Prepare Training Data\n",
        "\n",
        "Using `tokenizer.apply_chat_template()` with tools parameter - the proper way to format FunctionGemma prompts."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Training examples with thinking + tool calls\n",
        "training_examples = [\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a CLI tool recommendation assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"OS: darwin, Shell: zsh\\nQuery: find all python files\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"<think>User wants to find Python files. On macOS: find is installed by default (BSD). fd is a modern alternative.</think>\",\n",
        "                \"tool_calls\": [{\n",
        "                    \"id\": \"call_1\",\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": \"recommend_tools\",\n",
        "                        \"arguments\": {\n",
        "                            \"primary_tools\": [{\"name\": \"find\", \"category\": \"file_management\", \"confidence\": 1.0, \"reason\": \"Standard POSIX tool\"}],\n",
        "                            \"alternative_tools\": [{\"name\": \"fd\", \"install_cmd\": \"brew install fd\", \"reason\": \"Modern find alternative\", \"improvements\": [\"faster\", \"simpler\"]}],\n",
        "                            \"task_category\": \"file_management\"\n",
        "                        }\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a CLI tool recommendation assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"OS: ubuntu, Shell: bash\\nQuery: search for error in log files\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"<think>User wants text search. On Ubuntu: grep is installed by default. ripgrep is faster but needs install.</think>\",\n",
        "                \"tool_calls\": [{\n",
        "                    \"id\": \"call_1\",\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": \"recommend_tools\",\n",
        "                        \"arguments\": {\n",
        "                            \"primary_tools\": [{\"name\": \"grep\", \"category\": \"search\", \"confidence\": 1.0, \"reason\": \"Standard GNU grep\"}],\n",
        "                            \"alternative_tools\": [{\"name\": \"rg\", \"install_cmd\": \"apt install ripgrep\", \"reason\": \"Fastest grep\", \"improvements\": [\"faster\", \"gitignore\"]}],\n",
        "                            \"task_category\": \"search\"\n",
        "                        }\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a CLI tool recommendation assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"OS: darwin, Shell: zsh\\nQuery: parse JSON API response\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"<think>User needs JSON parsing. curl is on macOS by default. jq is the standard for JSON but needs install.</think>\",\n",
        "                \"tool_calls\": [{\n",
        "                    \"id\": \"call_1\",\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": \"recommend_tools\",\n",
        "                        \"arguments\": {\n",
        "                            \"primary_tools\": [{\"name\": \"curl\", \"category\": \"network\", \"confidence\": 1.0, \"reason\": \"Built-in HTTP client\"}],\n",
        "                            \"alternative_tools\": [{\"name\": \"jq\", \"install_cmd\": \"brew install jq\", \"reason\": \"JSON processor\", \"improvements\": [\"query syntax\", \"filtering\"]}],\n",
        "                            \"task_category\": \"json_processing\"\n",
        "                        }\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a CLI tool recommendation assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"OS: linux, Shell: bash\\nQuery: check disk usage\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"<think>User wants disk usage info. du is standard POSIX. ncdu is interactive alternative.</think>\",\n",
        "                \"tool_calls\": [{\n",
        "                    \"id\": \"call_1\",\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": \"recommend_tools\",\n",
        "                        \"arguments\": {\n",
        "                            \"primary_tools\": [{\"name\": \"du\", \"category\": \"disk\", \"confidence\": 1.0, \"reason\": \"Standard POSIX tool\"}],\n",
        "                            \"alternative_tools\": [{\"name\": \"ncdu\", \"install_cmd\": \"apt install ncdu\", \"reason\": \"Interactive disk analyzer\", \"improvements\": [\"interactive\", \"visual\"]}],\n",
        "                            \"task_category\": \"disk\"\n",
        "                        }\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a CLI tool recommendation assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"OS: darwin, Shell: fish\\nQuery: monitor processes\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"<think>Process monitoring on macOS. top is always available. htop has better UI but needs brew.</think>\",\n",
        "                \"tool_calls\": [{\n",
        "                    \"id\": \"call_1\",\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": \"recommend_tools\",\n",
        "                        \"arguments\": {\n",
        "                            \"primary_tools\": [{\"name\": \"top\", \"category\": \"process\", \"confidence\": 1.0, \"reason\": \"Built-in process monitor\"}],\n",
        "                            \"alternative_tools\": [{\"name\": \"htop\", \"install_cmd\": \"brew install htop\", \"reason\": \"Interactive viewer\", \"improvements\": [\"colors\", \"mouse\", \"tree\"]}],\n",
        "                            \"task_category\": \"process\"\n",
        "                        }\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Format using apply_chat_template with tools\n",
        "def format_example(example):\n",
        "    chat_str = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tools=tools,\n",
        "        add_generation_prompt=False,\n",
        "        tokenize=False,\n",
        "    )\n",
        "    # Remove BOS if present\n",
        "    if chat_str.startswith(\"<bos>\"):\n",
        "        chat_str = chat_str[5:]\n",
        "    return chat_str\n",
        "\n",
        "formatted_data = [format_example(ex) for ex in training_examples]\n",
        "\n",
        "print(f\"Prepared {len(formatted_data)} training examples\")\n",
        "print(\"\\nSample formatted example:\")\n",
        "print(\"=\" * 50)\n",
        "print(formatted_data[0][:800] + \"...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "print(f\"Dataset size: {len(dataset)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Configure Trainer with train_on_responses_only\n",
        "\n",
        "This masks user inputs and only trains on assistant outputs - significantly improves quality."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=None,\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=10,\n",
        "        max_steps=500,  # Set to None for full epochs\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.001,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# IMPORTANT: Only train on assistant responses\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<start_of_turn>user\\n\",\n",
        "    response_part=\"<start_of_turn>model\\n\",\n",
        ")\n",
        "\n",
        "print(\"Trainer configured with train_on_responses_only\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Show Memory Stats"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Train"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Show stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\nTraining time: {trainer_stats.metrics['train_runtime']:.1f}s\")\n",
        "print(f\"Peak memory: {used_memory} GB ({round(used_memory/max_memory*100, 1)}%)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Test Inference"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def get_recommendation(query, os_type=\"darwin\", shell=\"zsh\"):\n",
        "    \"\"\"Get CLI tool recommendations using apply_chat_template.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a CLI tool recommendation assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"OS: {os_type}, Shell: {shell}\\nQuery: {query}\"},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tools=tools,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    model_response = response.split(\"<start_of_turn>model\\n\")[-1]\n",
        "    if \"<end_of_turn>\" in model_response:\n",
        "        model_response = model_response.split(\"<end_of_turn>\")[0]\n",
        "\n",
        "    return model_response\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    (\"find all javascript files\", \"darwin\", \"zsh\"),\n",
        "    (\"compress a folder\", \"ubuntu\", \"bash\"),\n",
        "    (\"view running processes\", \"linux\", \"fish\"),\n",
        "]\n",
        "\n",
        "for query, os_type, shell in test_queries:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Query: {query} (OS: {os_type}, Shell: {shell})\")\n",
        "    print(\"=\"*50)\n",
        "    result = get_recommendation(query, os_type, shell)\n",
        "    print(result)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Save Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"cli-tool-recommender-lora\")\n",
        "tokenizer.save_pretrained(\"cli-tool-recommender-lora\")\n",
        "print(\"Model saved to cli-tool-recommender-lora/\")\n",
        "\n",
        "# Optional: Save merged 16-bit model\n",
        "# model.save_pretrained_merged(\"cli-tool-recommender-merged\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Optional: Save as GGUF for llama.cpp (only Q8_0, BF16, F16 supported)\n",
        "# model.save_pretrained_gguf(\"cli-tool-recommender-gguf\", tokenizer, quantization_method=\"Q8_0\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Download Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r cli-tool-recommender-lora.zip cli-tool-recommender-lora/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('cli-tool-recommender-lora.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
