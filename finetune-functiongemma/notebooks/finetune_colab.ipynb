{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FunctionGemma CLI Tool Recommender\n",
        "\n",
        "Fine-tune FunctionGemma (270M) to recommend CLI tools based on:\n",
        "- User queries\n",
        "- Operating system (macOS, Linux, Ubuntu, Windows, etc.)\n",
        "- Shell type (bash, zsh, fish, etc.)\n",
        "- User preferences\n",
        "\n",
        "**Requirements:** Free Colab T4 GPU"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install datasets trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Model and Tokenizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/functiongemma-270m-it\",\n",
        "    max_seq_length=4096,\n",
        "    load_in_16bit=True,\n",
        "    full_finetuning=False,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model.config._name_or_path}\")\n",
        "print(f\"Parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Add LoRA Adapters"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters applied\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare Training Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Sample training data (CLI tool recommendations)\n",
        "training_examples = [\n",
        "    {\n",
        "        \"query\": \"find all python files in the current directory\",\n",
        "        \"os\": \"darwin\",\n",
        "        \"shell\": \"zsh\",\n",
        "        \"thinking\": \"The user wants to find Python files. On macOS with zsh: find is installed by default (BSD version). fd is a modern alternative but not installed by default.\",\n",
        "        \"response\": {\n",
        "            \"primary_tools\": [{\"name\": \"find\", \"category\": \"file_management\", \"confidence\": 1.0, \"reason\": \"Standard POSIX tool installed by default on macOS\"}],\n",
        "            \"alternative_tools\": [{\"name\": \"fd\", \"category\": \"file_management\", \"install_cmd\": \"brew install fd\", \"reason\": \"Modern find alternative\", \"improvements\": [\"faster\", \"simpler syntax\"]}],\n",
        "            \"task_category\": \"file_management\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"search for the word error in all log files\",\n",
        "        \"os\": \"ubuntu\",\n",
        "        \"shell\": \"bash\",\n",
        "        \"thinking\": \"The user wants to search for text in log files. On Ubuntu: grep is installed by default (GNU grep). ripgrep would be faster but requires installation.\",\n",
        "        \"response\": {\n",
        "            \"primary_tools\": [{\"name\": \"grep\", \"category\": \"search\", \"confidence\": 1.0, \"reason\": \"Standard GNU tool installed by default\"}],\n",
        "            \"alternative_tools\": [{\"name\": \"rg\", \"category\": \"search\", \"install_cmd\": \"apt install ripgrep\", \"reason\": \"Fastest grep alternative\", \"improvements\": [\"faster\", \"respects gitignore\"]}],\n",
        "            \"task_category\": \"search\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"view a JSON API response and extract specific fields\",\n",
        "        \"os\": \"darwin\",\n",
        "        \"shell\": \"zsh\",\n",
        "        \"thinking\": \"The user needs to parse JSON. curl is on macOS by default for fetching. jq is needed for parsing but not installed by default.\",\n",
        "        \"response\": {\n",
        "            \"primary_tools\": [{\"name\": \"curl\", \"category\": \"network\", \"confidence\": 1.0, \"reason\": \"Standard HTTP client installed by default\"}],\n",
        "            \"alternative_tools\": [{\"name\": \"jq\", \"category\": \"json_processing\", \"install_cmd\": \"brew install jq\", \"reason\": \"Powerful JSON processor\", \"improvements\": [\"query syntax\", \"filtering\", \"transformation\"]}],\n",
        "            \"task_category\": \"json_processing\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"show disk usage of my home directory\",\n",
        "        \"os\": \"linux\",\n",
        "        \"shell\": \"bash\",\n",
        "        \"thinking\": \"User wants to see disk usage. du is standard POSIX tool. ncdu is a better interactive alternative.\",\n",
        "        \"response\": {\n",
        "            \"primary_tools\": [{\"name\": \"du\", \"category\": \"disk\", \"confidence\": 1.0, \"reason\": \"Standard POSIX tool for directory space usage\"}],\n",
        "            \"alternative_tools\": [{\"name\": \"ncdu\", \"category\": \"disk\", \"install_cmd\": \"apt install ncdu\", \"reason\": \"Interactive disk usage analyzer\", \"improvements\": [\"interactive UI\", \"visual browsing\"]}],\n",
        "            \"task_category\": \"disk\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"monitor system processes and CPU usage\",\n",
        "        \"os\": \"darwin\",\n",
        "        \"shell\": \"fish\",\n",
        "        \"thinking\": \"Process monitoring on macOS. top is always available. htop provides better UI but needs installation.\",\n",
        "        \"response\": {\n",
        "            \"primary_tools\": [{\"name\": \"top\", \"category\": \"process\", \"confidence\": 1.0, \"reason\": \"Built-in process monitor\"}],\n",
        "            \"alternative_tools\": [{\"name\": \"htop\", \"category\": \"process\", \"install_cmd\": \"brew install htop\", \"reason\": \"Interactive process viewer\", \"improvements\": [\"colored output\", \"mouse support\", \"tree view\"]}],\n",
        "            \"task_category\": \"process\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def format_example(example):\n",
        "    \"\"\"Format a single example for FunctionGemma.\"\"\"\n",
        "    system = \"You are a CLI tool recommendation assistant. Given the user's query, OS, shell, and preferences, recommend the most appropriate CLI tools. Format your response as a function call to recommend_tools.\"\n",
        "\n",
        "    user_msg = f\"OS: {example['os']}, Shell: {example['shell']}\\nQuery: {example['query']}\"\n",
        "\n",
        "    model_response = f\"<think>\\n{example['thinking']}\\n</think>\\n\"\n",
        "    model_response += f\"<start_function_call>call:recommend_tools{json.dumps(example['response'], indent=2)}<end_function_call>\"\n",
        "\n",
        "    formatted = f\"<start_of_turn>developer\\n{system}<end_of_turn>\\n\"\n",
        "    formatted += f\"<start_of_turn>user\\n{user_msg}<end_of_turn>\\n\"\n",
        "    formatted += f\"<start_of_turn>model\\n{model_response}<end_of_turn>\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "# Format all examples\n",
        "formatted_data = [format_example(ex) for ex in training_examples]\n",
        "\n",
        "print(f\"Prepared {len(formatted_data)} training examples\")\n",
        "print(\"\\nSample formatted example:\")\n",
        "print(\"=\" * 50)\n",
        "print(formatted_data[0][:500] + \"...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Create Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "print(f\"Dataset size: {len(dataset)}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Configure Trainer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=4096,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./output\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Trainer configured\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Train"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test Inference"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def get_recommendation(query, os_type=\"darwin\", shell=\"zsh\"):\n",
        "    \"\"\"Get CLI tool recommendations.\"\"\"\n",
        "    system = \"You are a CLI tool recommendation assistant. Given the user's query, OS, shell, and preferences, recommend the most appropriate CLI tools. Format your response as a function call to recommend_tools.\"\n",
        "\n",
        "    prompt = f\"<start_of_turn>developer\\n{system}<end_of_turn>\\n\"\n",
        "    prompt += f\"<start_of_turn>user\\nOS: {os_type}, Shell: {shell}\\nQuery: {query}<end_of_turn>\\n\"\n",
        "    prompt += \"<start_of_turn>model\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    # Extract model response\n",
        "    model_response = response.split(\"<start_of_turn>model\\n\")[-1]\n",
        "    if \"<end_of_turn>\" in model_response:\n",
        "        model_response = model_response.split(\"<end_of_turn>\")[0]\n",
        "\n",
        "    return model_response\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"find all javascript files\",\n",
        "    \"compress a folder into a zip file\",\n",
        "    \"view running processes\",\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"=\"*50)\n",
        "    result = get_recommendation(query)\n",
        "    print(result)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA adapters\n",
        "model.save_pretrained(\"cli-tool-recommender-lora\")\n",
        "tokenizer.save_pretrained(\"cli-tool-recommender-lora\")\n",
        "\n",
        "print(\"Model saved to cli-tool-recommender-lora/\")\n",
        "\n",
        "# Optional: Save merged 16-bit model\n",
        "# model.save_pretrained_merged(\"cli-tool-recommender-merged\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Optional: Save as GGUF for llama.cpp\n",
        "# model.save_pretrained_gguf(\"cli-tool-recommender-gguf\", tokenizer, quantization_method=\"q4_k_m\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Download Model\n",
        "\n",
        "Run this cell to download your trained model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r cli-tool-recommender-lora.zip cli-tool-recommender-lora/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('cli-tool-recommender-lora.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
