# Implementation Plan: LLM Evaluation Harness for Shell Commands

**Branch**: `022-issue-135-build` | **Date**: 2026-01-08 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/kitty-specs/022-issue-135-build/spec.md`

**Note**: This plan was generated by the `/spec-kitty.plan` command. All planning questions have been answered using project best practices and spec assumptions.

## Summary

Build a comprehensive evaluation framework to measure LLM-generated shell command quality across correctness, safety, and POSIX compliance dimensions. The harness will use the existing caro CLI as a black-box interface, execute test datasets through configured backends (MLX, Ollama, vLLM), and generate detailed reports showing accuracy metrics, safety detection rates, and backend performance comparisons. Initial focus is on automated testing infrastructure with manual test dataset curation; dashboard visualization deferred to v1.2.0.

## Technical Context

**Language/Version**: Rust 1.83 (current caro version)
**Primary Dependencies**:
- serde/serde_json - Test dataset serialization and result reporting
- tokio - Async test execution and parallel processing
- clap - Evaluation CLI argument parsing
- indicatif - Progress bars during test runs
- shellcheck (external) - POSIX compliance validation
- similar - Text diff generation for command comparison

**Storage**:
- Test datasets: JSON files in `tests/evaluation/datasets/` (version-controlled)
- Evaluation results: JSON + Markdown files in `tests/evaluation/results/` (timestamped, gitignored)

**Testing**: cargo test with dedicated `tests/evaluation/` module
**Target Platform**: Linux, macOS (primary development), Windows (future)
**Project Type**: Single Rust project (library + CLI tool)

**Performance Goals**:
- Complete 100-prompt evaluation in < 5 minutes (SC-001)
- Parallel test execution across backends
- Sub-second individual test case execution
- Minimal memory overhead (< 500MB for full test run)

**Constraints**:
- Must use existing caro CLI (black-box, no direct backend API calls)
- POSIX validation requires shellcheck external dependency
- Semantic equivalence rules start simple, expand iteratively
- No network dependencies for offline testing capability

**Scale/Scope**:
- Initial: 100-200 manually curated test cases
- Target (v1.1.0): 500+ test cases covering all command categories
- MVP: Correctness + Safety validation (User Stories 1-2, Priority P1)
- Phase 2: POSIX compliance + Backend comparison (User Stories 3-4, Priority P2)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

Based on caro project principles from CLAUDE.md:

✅ **Safety-First Design**: Evaluation harness validates safety module effectiveness, directly supports core principle
✅ **POSIX Compliance**: Harness verifies POSIX compliance requirement through dedicated validation
✅ **Testing Standards**: Uses cargo test framework, follows existing test structure (integration/, unit/)
✅ **Performance Requirements**: < 5 min for 100 prompts meets project's efficiency standards
✅ **Backend Abstraction**: Tests all backends (MLX, Ollama, vLLM) through unified interface
✅ **Single Binary**: Evaluation tool will be cargo test-based, no separate binary required
✅ **Offline Capability**: Test datasets version-controlled, no network dependencies for test execution

**No constitution violations identified. GATE PASSED.**

## Project Structure

### Documentation (this feature)

```
kitty-specs/[###-feature]/
├── plan.md              # This file (/spec-kitty.plan command output)
├── research.md          # Phase 0 output (/spec-kitty.plan command)
├── data-model.md        # Phase 1 output (/spec-kitty.plan command)
├── quickstart.md        # Phase 1 output (/spec-kitty.plan command)
├── contracts/           # Phase 1 output (/spec-kitty.plan command)
└── tasks.md             # Phase 2 output (/spec-kitty.tasks command - NOT created by /spec-kitty.plan)
```

### Source Code (repository root)

```
tests/
├── evaluation/                    # NEW: LLM evaluation harness
│   ├── datasets/                  # Test case collections
│   │   ├── correctness/           # Command correctness tests
│   │   │   ├── file_operations.json
│   │   │   ├── text_processing.json
│   │   │   └── network_commands.json
│   │   ├── safety/                # Safety detection tests
│   │   │   ├── dangerous_patterns.json
│   │   │   └── false_positives.json
│   │   ├── posix/                 # POSIX compliance tests
│   │   │   ├── bash_specific.json
│   │   │   └── portable_commands.json
│   │   └── backend_comparison/    # Multi-backend consistency
│   │       └── cross_backend.json
│   │
│   ├── src/                       # Evaluation library code
│   │   ├── lib.rs                 # Module exports
│   │   ├── dataset.rs             # TestCase, TestDataset types
│   │   ├── executor.rs            # CLI invocation, result capture
│   │   ├── evaluator.rs           # Correctness scoring logic
│   │   ├── safety_validator.rs    # Safety detection validation
│   │   ├── posix_checker.rs       # POSIX compliance checking
│   │   ├── reporter.rs            # JSON/Markdown report generation
│   │   └── equivalence.rs         # Semantic equivalence rules
│   │
│   ├── tests/                     # Integration tests for harness
│   │   ├── test_correctness.rs    # FR-001, FR-003 validation
│   │   ├── test_safety.rs         # FR-004 validation
│   │   ├── test_posix.rs          # FR-005 validation
│   │   └── test_backends.rs       # FR-008, FR-009 validation
│   │
│   ├── results/                   # Output directory (gitignored)
│   │   └── .gitkeep
│   │
│   ├── Cargo.toml                 # Evaluation harness dependencies
│   └── README.md                  # Usage documentation
│
├── integration/                   # Existing caro integration tests
└── unit/                          # Existing caro unit tests

src/                               # Existing caro source (unchanged)
├── backends/
├── safety/                        # Used by evaluation harness
├── cache/
└── config/
```

**Structure Decision**: Single project structure with new `tests/evaluation/` subdirectory. This follows existing caro test organization (integration/, unit/) and keeps evaluation code separate from production caro source. The evaluation harness is a testing tool, not a runtime feature, so it lives entirely under tests/.

**Key Design Points**:
1. **Isolation**: Evaluation harness in tests/evaluation/ doesn't affect production binary
2. **Reusability**: src/ modules can be imported by tests/integration/ if needed
3. **Dataset Organization**: Subdirectories by test category (correctness, safety, posix, backend_comparison)
4. **Results Gitignored**: timestamped evaluation results excluded from version control
5. **Cargo Workspace**: tests/evaluation/Cargo.toml allows independent dependency management

## Complexity Tracking

*No constitution violations - section not applicable.*

## Implementation Phases

### Phase 0: Research & Design Decisions

All technical decisions have been resolved through spec assumptions and existing caro patterns:

**Dataset Format** (resolved):
- Decision: JSON files with schema: `{ "prompt": string, "expected_command": string, "category": string, "risk_level": string, "posix_compliant": bool }`
- Rationale: Standard format, easy to parse with serde_json, version-controllable
- Alternative: YAML (rejected - less tooling support in Rust ecosystem)

**CLI Invocation Strategy** (resolved):
- Decision: Use `std::process::Command` to invoke caro binary as black-box
- Rationale: Follows spec assumption (FR-002), maintains separation of concerns
- Alternative: Direct backend API calls (rejected - violates black-box requirement)

**POSIX Validation Tool** (resolved):
- Decision: Use shellcheck via subprocess for static analysis
- Rationale: Industry-standard tool, comprehensive ruleset, actively maintained
- Alternative: Custom parser (rejected - massive scope, reinventing wheel)

**Semantic Equivalence Rules** (resolved):
- Decision: Start with exact match + basic normalization (whitespace, flag ordering)
- Rationale: Spec assumption states "start simple, expand iteratively"
- Alternative: Full AST parsing (deferred - Phase 2 enhancement)

**Performance Optimization** (resolved):
- Decision: Tokio async runtime with parallel test execution via spawn tasks
- Rationale: Meets < 5 min target for 100 prompts, leverages existing tokio dependency
- Alternative: Sequential execution (rejected - too slow for large datasets)

### Phase 1: Data Model & Contracts

**Data Model Created**: See [data-model.md](data-model.md) for complete entity definitions.

**Key Entities**:
- TestCase, TestDataset - Input test data structures
- EvaluationResult, EvaluationRun - Output evaluation data
- SafetyValidationResult, PosixValidationResult - Validation outcomes
- BackendMetrics, EvaluationSummary - Aggregated statistics

**No API Contracts**: This is a testing tool, not a service. All interactions are:
1. Load datasets from JSON files (FR-001)
2. Execute caro CLI via `std::process::Command` (FR-002)
3. Write results to JSON/Markdown files (FR-006)

**Agent Context**: Updated with evaluation harness-specific dependencies and structure.

## Planning Complete

**Branch**: `022-issue-135-build`
**Artifacts Generated**:
- ✅ plan.md (this file)
- ✅ data-model.md (15 entities with relationships and validation rules)
- ✅ quickstart.md (5 integration scenarios + troubleshooting)

**Next Step**: Run `/spec-kitty.tasks` to generate work packages and implementation prompts.

**Constitutional Compliance**: All gates passed. No complexity violations.

**Ready for Task Generation**: Yes
