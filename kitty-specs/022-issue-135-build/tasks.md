---
description: "Work package task list for LLM Evaluation Harness (Issue #135)"
---

# Work Packages: LLM Evaluation Harness for Shell Commands

**Inputs**: Design documents from `/kitty-specs/022-issue-135-build/`
**Prerequisites**: plan.md, spec.md, data-model.md, quickstart.md

**Organization**: Fine-grained subtasks (`Txxx`) roll up into work packages (`WPxx`). Each work package is independently deliverable and testable.

**Prompt Files**: Each work package references a matching prompt file in `/tasks/planned/` generated by `/spec-kitty.tasks`.

## Subtask Format: `[Txxx] [P?] Description`
- **[P]** indicates the subtask can proceed in parallel (different files/components).
- Paths relative to repository root

---

## Work Package WP01: Setup & Infrastructure (Priority: P0)

**Goal**: Establish evaluation harness project structure, dependencies, and foundational types.
**Independent Test**: Project structure exists, dependencies compile, basic types defined.
**Prompt**: `/tasks/planned/WP01-setup-infrastructure.md`

### Included Subtasks
- [ ] T001 Create `tests/evaluation/` directory structure
- [ ] T002 Create `tests/evaluation/Cargo.toml` with dependencies
- [ ] T003 Create `tests/evaluation/src/lib.rs` with module exports
- [ ] T004 [P] Create `.gitignore` entry for `tests/evaluation/results/`
- [ ] T005 [P] Create dataset directory structure under `tests/evaluation/datasets/`
- [ ] T006 Create `tests/evaluation/README.md` with usage documentation

### Implementation Notes
- Directory structure: datasets/{correctness,safety,posix,backend_comparison}/
- Dependencies: serde, serde_json, tokio, clap, indicatif, similar
- External dependency: shellcheck (document installation requirements)

### Parallel Opportunities
- T004 (gitignore) and T005 (directories) and T006 (README) can proceed in parallel after T001

### Dependencies
- None (starting package)

### Risks & Mitigations
- Cargo workspace conflicts â†’ use separate Cargo.toml for evaluation harness
- Shellcheck not installed â†’ document prerequisite in README

---

## Work Package WP02: Test Dataset Structure & Loader (Priority: P0)

**Goal**: Define data model types and implement JSON dataset loading.
**Independent Test**: Load sample dataset JSON file, deserialize TestCase/TestDataset types successfully.
**Prompt**: `/tasks/planned/WP02-dataset-loader.md`

### Included Subtasks
- [ ] T007 Implement `tests/evaluation/src/dataset.rs` with TestCase struct
- [ ] T008 Implement TestDataset struct with serde deserialization
- [ ] T009 Implement TestCaseMetadata and DatasetMetadata types
- [ ] T010 Add dataset loading function with error handling
- [ ] T011 Create sample test dataset `tests/evaluation/datasets/correctness/file_operations.json`
- [ ] T012 Add unit tests for dataset deserialization in `tests/evaluation/src/dataset.rs`

### Implementation Notes
- TestCase schema: { id, prompt, expected_command, category, risk_level, posix_compliant, tags, metadata }
- Implement From/Into for risk level string <-> enum conversion
- Validation: unique IDs, non-empty prompts, valid categories

### Parallel Opportunities
- T011 (sample dataset) can be created in parallel with T007-T010 implementation

### Dependencies
- Depends on WP01 (project structure exists)

### Risks & Mitigations
- JSON schema drift â†’ add JSON schema file for validation
- Large datasets slow loading â†’ defer optimization until needed

---

## Work Package WP03: Command Correctness Evaluation (Priority: P1) ðŸŽ¯ MVP

**Goal**: Implement command correctness validation (User Story 1) with exact match and semantic equivalence.
**Independent Test**: Run 20-prompt correctness dataset, generate accuracy report showing match rate.
**Prompt**: `/tasks/planned/WP03-correctness-evaluation.md`

### Included Subtasks
- [ ] T013 Implement `tests/evaluation/src/executor.rs` for CLI invocation via std::process::Command
- [ ] T014 Implement command output capture and error handling
- [ ] T015 Implement `tests/evaluation/src/evaluator.rs` with exact match logic
- [ ] T016 Implement semantic equivalence rules (whitespace normalization, flag ordering)
- [ ] T017 Implement correctness scoring (0.0-1.0) with match method classification
- [ ] T018 Implement diff generation using `similar` crate for mismatches
- [ ] T019 Create correctness test datasets: file_operations.json, text_processing.json, network_commands.json
- [ ] T020 Add unit tests for equivalence rules
- [ ] T021 Add integration test `tests/evaluation/tests/test_correctness.rs`

### Implementation Notes
- Executor spawns `cargo run -- <prompt>` and captures stdout
- Evaluator compares generated vs expected using exact match first, then equivalence rules
- Diff shows side-by-side comparison for failures

### Parallel Opportunities
- T019 (dataset creation) can proceed in parallel with T013-T018 implementation
- T020 (unit tests) and T021 (integration test) can proceed in parallel after core implementation

### Dependencies
- Depends on WP02 (dataset loader exists)

### Risks & Mitigations
- CLI invocation timeout â†’ set 30s timeout per command
- Semantic equivalence too broad â†’ start conservative, expand iteratively
- Dataset quality â†’ manually review all test cases for correctness

---

## Work Package WP04: Safety Pattern Validation (Priority: P1) ðŸŽ¯ MVP

**Goal**: Implement safety detection validation (User Story 2) with confusion matrix calculation.
**Independent Test**: Run 50-prompt safety dataset, generate precision/recall report.
**Prompt**: `/tasks/planned/WP04-safety-validation.md`

### Included Subtasks
- [ ] T022 Implement `tests/evaluation/src/safety_validator.rs` module
- [ ] T023 Extract safety patterns from `src/safety/` module for validation
- [ ] T024 Implement safety pattern matching against generated commands
- [ ] T025 Implement confusion matrix calculation (TP, FP, TN, FN)
- [ ] T026 Implement precision, recall, F1-score metrics
- [ ] T027 Implement false positive/negative detection using ground truth labels
- [ ] T028 Create safety test datasets: dangerous_patterns.json, false_positives.json
- [ ] T029 Add unit tests for safety validation logic
- [ ] T030 Add integration test `tests/evaluation/tests/test_safety.rs`

### Implementation Notes
- Load safety patterns from src/safety/ (dangerous commands, risk levels)
- Ground truth: TestCase.risk_level provides expected classification
- Confusion matrix: compare caro's classification vs ground truth

### Parallel Opportunities
- T028 (datasets) can proceed in parallel with T022-T027 implementation
- T029 (unit tests) and T030 (integration test) can proceed after core logic

### Dependencies
- Depends on WP02 (dataset loader)
- Uses WP03 executor for CLI invocation

### Risks & Mitigations
- Safety module API changes â†’ use stable public interface only
- False positive rate too high â†’ document findings, defer fixes to safety module improvements

---

## Work Package WP05: POSIX Compliance Checker (Priority: P2)

**Goal**: Implement POSIX compliance validation (User Story 3) using shellcheck.
**Independent Test**: Run 20-prompt POSIX dataset, identify bash-specific syntax violations.
**Prompt**: `/tasks/planned/WP05-posix-compliance.md`

### Included Subtasks
- [ ] T031 Implement `tests/evaluation/src/posix_checker.rs` module
- [ ] T032 Implement shellcheck subprocess invocation and output parsing
- [ ] T033 Implement bash-specific syntax detection from shellcheck warnings
- [ ] T034 Implement POSIX compliance scoring based on violation count
- [ ] T035 Implement portable alternative suggestions (if shellcheck provides them)
- [ ] T036 Create POSIX test datasets: bash_specific.json, portable_commands.json
- [ ] T037 Add unit tests for shellcheck parsing
- [ ] T038 Add integration test `tests/evaluation/tests/test_posix.rs`

### Implementation Notes
- Requires shellcheck external dependency (check in PATH)
- Parse shellcheck JSON output format for structured violation data
- Common violations: [[ ]], arrays, process substitution

### Parallel Opportunities
- T036 (datasets) can proceed in parallel with T031-T035 implementation
- T037 (unit tests) and T038 (integration test) can proceed after core logic

### Dependencies
- Depends on WP02 (dataset loader)
- Uses WP03 executor for command generation

### Risks & Mitigations
- Shellcheck not installed â†’ fail gracefully with clear error message, document prereq
- Shellcheck version differences â†’ pin minimum version requirement (0.8.0+)

---

## Work Package WP06: Multi-Backend Comparison (Priority: P2)

**Goal**: Implement backend comparison analysis (User Story 4) with statistical significance testing.
**Independent Test**: Run same 50-prompt dataset through MLX/Ollama/vLLM, generate variance report.
**Prompt**: `/tasks/planned/WP06-backend-comparison.md`

### Included Subtasks
- [ ] T039 Extend executor to support backend selection via environment variable
- [ ] T040 Implement backend configuration loading from caro config
- [ ] T041 Implement parallel execution across multiple backends
- [ ] T042 Implement backend-specific result aggregation
- [ ] T043 Implement statistical comparison (mean, std dev, confidence intervals)
- [ ] T044 Implement variance detection and significance testing
- [ ] T045 Create backend comparison dataset: cross_backend.json
- [ ] T046 Add integration test `tests/evaluation/tests/test_backends.rs`

### Implementation Notes
- Test same prompts through each configured backend
- Aggregate results by backend_name for comparison
- Statistical significance: chi-square test for categorical (correctness), t-test for continuous (latency)

### Parallel Opportunities
- T045 (dataset) can proceed in parallel with implementation
- Backend executions (T041) can run in parallel via tokio spawn

### Dependencies
- Depends on WP03 (correctness evaluator), WP04 (safety validator), WP05 (POSIX checker)

### Risks & Mitigations
- Backends not configured â†’ skip unavailable backends with warning
- Statistical power low â†’ require minimum 30 samples per backend for significance tests

---

## Work Package WP07: Report Generation & Integration Tests (Priority: P3)

**Goal**: Implement JSON/Markdown report generation and end-to-end integration tests.
**Independent Test**: Run full evaluation, generate timestamped report files, verify format.
**Prompt**: `/tasks/planned/WP07-reporting-integration.md`

### Included Subtasks
- [ ] T047 Implement `tests/evaluation/src/reporter.rs` module
- [ ] T048 Implement EvaluationResult struct serialization
- [ ] T049 Implement EvaluationSummary aggregation from results
- [ ] T050 Implement BackendMetrics calculation
- [ ] T051 Implement JSON report generation to `tests/evaluation/results/run_<timestamp>.json`
- [ ] T052 Implement Markdown report generation with tables and formatting
- [ ] T053 Add performance metrics capture (latency, memory usage)
- [ ] T054 Add timestamp and git commit tracking to reports
- [ ] T055 Create full end-to-end integration test that runs all work packages
- [ ] T056 Add CLI wrapper script for easy evaluation execution

### Implementation Notes
- JSON format follows data-model.md schema
- Markdown format includes summary tables, per-category breakdowns, regressions
- Capture git commit via `git rev-parse HEAD`

### Parallel Opportunities
- T051 (JSON) and T052 (Markdown) can proceed in parallel
- T056 (CLI wrapper) can proceed in parallel with report implementation

### Dependencies
- Depends on all previous work packages (WP03-WP06 evaluation logic)

### Risks & Mitigations
- Report file size too large â†’ limit detailed output, use summary aggregations
- Markdown formatting breaks in different viewers â†’ use GitHub-flavored markdown standard

---

## Execution Plan

### Phase 1: Foundation (WP01-WP02)
Sequential execution required. Complete setup before proceeding to evaluation logic.

**Estimated Effort**: 4-6 hours
**Deliverable**: Project structure, dataset types, sample datasets

### Phase 2: MVP Evaluation (WP03-WP04) ðŸŽ¯
Parallel execution possible after foundation complete. These deliver the P1 user stories.

**Estimated Effort**: 12-16 hours
**Deliverable**: Correctness and safety validation functional, 100+ test cases

### Phase 3: Extended Validation (WP05-WP06)
Can proceed in parallel. These deliver P2 user stories.

**Estimated Effort**: 8-12 hours
**Deliverable**: POSIX compliance and backend comparison functional

### Phase 4: Polish (WP07)
Final integration and reporting layer.

**Estimated Effort**: 6-8 hours
**Deliverable**: Complete evaluation reports, end-to-end tests

**Total Estimated Effort**: 30-42 hours (3.75-5.25 days at 8 hours/day)

---

## MVP Scope Recommendation

**Minimum Viable Product**: WP01, WP02, WP03, WP04

This delivers:
- âœ… Command correctness validation (User Story 1, P1)
- âœ… Safety pattern detection accuracy (User Story 2, P1)
- âœ… Automated test execution infrastructure
- âœ… Basic JSON reporting
- âœ… Success criteria SC-001, SC-002, SC-003, SC-006 validated

**Deferred to Phase 2**: WP05 (POSIX), WP06 (backends), WP07 (advanced reporting)

---

## Notes

- **Test datasets require manual curation**: Allocate time for creating high-quality test cases
- **Shellcheck dependency**: Document installation instructions for Linux/macOS/Windows
- **Backend availability**: Evaluation harness adapts to configured backends, skips unavailable ones
- **Performance target**: < 5 minutes for 100 prompts (SC-001) - verify during WP03-WP04 implementation
- **Dashboard (User Story 5)**: Explicitly deferred to v1.2.0 per spec assumptions
