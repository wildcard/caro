---
description: "Work package task list for LLM Evaluation Harness implementation"
---

# Work Packages: LLM Evaluation Harness

**Inputs**: Design documents from `/kitty-specs/026-llm-evaluation-harness/`
**Prerequisites**: plan.md âœ…, spec.md âœ…, research.md âœ…, data-model.md âœ…, contracts/evaluation-api.md âœ…, quickstart.md âœ…

**Organization**: Fine-grained subtasks (`Txxx`) roll up into work packages (`WPxx`). Each work package must be independently deliverable and testable.

**Prompt Files**: Each work package references a matching prompt file in `/tasks/planned/` generated by `/spec-kitty.tasks`. Treat this file as the high-level checklist; keep deep implementation detail inside the prompt files.

## Subtask Format: `[Txxx] [P?] Description`
- **[P]** indicates the subtask can proceed in parallel (different files/components).
- Include precise file paths or modules.

## Path Conventions
- **Single project** (caro): `src/evaluation/`, `tests/evaluation/`
- All paths relative to repository root

---

## Work Package WP01: Core Models & Dataset Infrastructure (Priority: P0)

**Goal**: Establish data models and YAML dataset loading infrastructure that all evaluators depend on.
**Independent Test**: Can load a sample YAML dataset and validate schema; models serialize to/from JSON correctly.
**Prompt**: `/tasks/planned/WP01-core-models-and-dataset.md`

### Included Subtasks
- [ ] T001 Create `src/evaluation/` module structure with mod.rs
- [ ] T002 [P] Define core data models in `src/evaluation/models.rs` (TestCase, TestCategory, ValidationRule, EvaluationResult, BenchmarkReport, etc.)
- [ ] T003 [P] Implement Error types in `src/evaluation/mod.rs` (EvaluationError enum with thiserror)
- [ ] T004 Create sample `tests/evaluation/dataset.yaml` with schema structure
- [ ] T005 Implement dataset loading in `src/evaluation/dataset.rs` with serde_yaml
- [ ] T006 Add dataset validation (schema checking, unique IDs, required fields, category validation)
- [ ] T007 [P] Unit tests for models (serialization/deserialization, validation rules)
- [ ] T008 [P] Unit tests for dataset loading (valid/invalid YAML, schema violations)

### Implementation Notes
1. Start with models.rs: Define all structs from data-model.md with serde derives
2. Implement dataset.rs: YAML parsing with comprehensive error messages
3. Add validation: Check for duplicate test IDs, invalid categories, missing required fields
4. Test thoroughly: Valid datasets load, invalid datasets fail with clear errors

### Parallel Opportunities
- T002 (models) and T003 (errors) can be developed independently
- T007 (model tests) and T008 (dataset tests) can be written in parallel once T002-T006 complete

### Dependencies
- None (foundation package)

### Risks & Mitigations
- **YAML parsing errors**: Provide detailed error messages with line numbers
- **Schema drift**: Document YAML schema clearly in dataset.yaml header comments
- **Validation gaps**: Comprehensive validation prevents runtime errors downstream

---

## Work Package WP02: Evaluator Trait & Implementations (Priority: P0)

**Goal**: Implement the Evaluator trait and all four category-specific evaluators (Correctness, Safety, POSIX, Consistency).
**Independent Test**: Each evaluator can be unit tested with mock TestCase and CommandResult data; produces correct EvaluationResults.
**Prompt**: `/tasks/planned/WP02-evaluator-implementations.md`

### Included Subtasks
- [ ] T009 Define `Evaluator` trait in `src/evaluation/mod.rs` with async_trait
- [ ] T010 Create evaluator utilities in `src/evaluation/utils.rs` (command equivalence checker, pattern matcher, POSIX validator helpers)
- [ ] T011 [P] Implement `CorrectnessEvaluator` in `src/evaluation/evaluators/correctness.rs`
- [ ] T012 [P] Unit tests for CorrectnessEvaluator with various validation rules (exact_match, command_equivalence, pattern_match)
- [ ] T013 [P] Implement `SafetyEvaluator` in `src/evaluation/evaluators/safety.rs` (wraps existing `src/safety/` module)
- [ ] T014 [P] Unit tests for SafetyEvaluator (must_be_blocked, must_execute scenarios)
- [ ] T015 [P] Implement `POSIXEvaluator` in `src/evaluation/evaluators/posix.rs`
- [ ] T016 [P] Unit tests for POSIXEvaluator (POSIX compliance checking, shell portability)
- [ ] T017 [P] Implement `ConsistencyEvaluator` in `src/evaluation/evaluators/consistency.rs` (multi-backend output comparison)
- [ ] T018 [P] Unit tests for ConsistencyEvaluator (backend output consistency detection)

### Implementation Notes
1. Define Evaluator trait: `async fn evaluate(&self, test_case: &TestCase, result: &CommandResult) -> Result<EvaluationResult>`
2. Implement utils.rs: Command equivalence (handles `find .` vs `find . -type f`), regex pattern matching
3. Four evaluators in parallel:
   - CorrectnessEvaluator: Uses utils for equivalence/pattern checking
   - SafetyEvaluator: Reuses `src/safety/patterns.rs` for consistency
   - POSIXEvaluator: Checks for non-POSIX syntax (bash-specific features, GNU-only flags)
   - ConsistencyEvaluator: Compares outputs from multiple backends
4. Comprehensive unit tests: Each evaluator tested independently with mock data

### Parallel Opportunities
- **HIGH PARALLELIZATION**: All four evaluators (T011-T018) can be implemented completely independently
- Different developers can own different evaluators
- Tests can be written in parallel with implementation

### Dependencies
- Depends on WP01 (needs models and TestCase/CommandResult structs)

### Risks & Mitigations
- **Command equivalence complexity**: Start simple (exact match, basic equivalence), iterate based on test failures
- **POSIX validation accuracy**: Use shellcheck patterns as reference, document edge cases
- **Consistency definition**: Define clear thresholds for "equivalent" outputs (exact vs functionally equivalent)

---

## Work Package WP03: Harness Orchestration & Parallel Execution (Priority: P1)

**Goal**: Implement EvaluationHarness that orchestrates parallel backend execution and aggregates results into BenchmarkReports.
**Independent Test**: Can run evaluation with test dataset across multiple backends; handles backend failures; completes in <5 minutes.
**Prompt**: `/tasks/planned/WP03-harness-orchestration.md`

### Included Subtasks
- [ ] T019 Implement `EvaluationHarness` struct in `src/evaluation/harness.rs`
- [ ] T020 Implement harness initialization (load dataset, register backends, register evaluators)
- [ ] T021 Implement parallel backend execution with tokio::spawn (per-backend concurrent evaluation)
- [ ] T022 Implement per-category evaluation filtering (`run_category` method)
- [ ] T023 Implement per-backend evaluation filtering (`run_backend` method)
- [ ] T024 Implement result aggregation into BenchmarkReport (calculate pass rates, category/backend breakdowns)
- [ ] T025 Implement backend failure handling (timeouts, crashes, unavailability detection)
- [ ] T026 Implement platform detection for backend availability (e.g., MLX requires macOS arm64)
- [ ] T027 Integration tests in `tests/integration/evaluation_tests.rs` (full evaluation runs with mock backends)
- [ ] T028 Performance validation: Ensure <5 min execution with 100 tests Ã— 4 backends

### Implementation Notes
1. Harness struct: Holds dataset, backends (Vec<Box<dyn Backend>>), evaluators (HashMap<TestCategory, Box<dyn Evaluator>>)
2. Parallel execution strategy:
   - Outer loop: Iterate test cases
   - Inner parallel: tokio::spawn for each backend
   - Timeout per backend: 30 seconds using tokio::time::timeout
   - Aggregate results as they complete
3. Failure handling:
   - Backend timeout â†’ EvaluationResult with error
   - Backend crash â†’ Log error, record failure, continue with other backends
   - Backend unavailable â†’ Skip gracefully, report in results
4. Platform detection: Check uname, required features, service availability

### Parallel Opportunities
- T025 (failure handling) and T026 (platform detection) can be implemented in parallel with T021-T024
- Integration tests (T027) can be written while implementation is in progress

### Dependencies
- Depends on WP01 (models, dataset)
- Depends on WP02 (evaluators)

### Risks & Mitigations
- **Performance regression**: T028 validates <5min target, may need backend sampling adjustments
- **Race conditions**: Use proper tokio synchronization (Arc, Mutex where needed)
- **Backend flakiness**: Comprehensive timeout and retry logic

---

## Work Package WP04: Baseline Storage & Regression Detection (Priority: P1)

**Goal**: Implement baseline comparison logic for regression detection in CI/CD workflows.
**Independent Test**: Can store baseline, load baseline, compare two BenchmarkReports, detect regressions with configurable threshold.
**Prompt**: `/tasks/planned/WP04-baseline-and-regression.md`

### Included Subtasks
- [ ] T029 Implement baseline storage in `src/evaluation/baseline.rs` (JSON serialization to `tests/evaluation/baselines/`)
- [ ] T030 Implement baseline loading with error handling (missing baseline, corrupt JSON)
- [ ] T031 Implement comparison logic (BaselineDelta calculation with per-category and per-backend deltas)
- [ ] T032 Implement regression detection with configurable threshold (default 5%)
- [ ] T033 Add regression reporting (significant_regressions list in BaselineDelta)
- [ ] T034 Unit tests for baseline CRUD operations (create, read, corrupted files)
- [ ] T035 Unit tests for comparison logic (positive/negative deltas, threshold crossing)
- [ ] T036 Integration tests for full baseline workflow (store â†’ load â†’ compare â†’ detect regression)

### Implementation Notes
1. Baseline storage: BenchmarkReport â†’ JSON at `tests/evaluation/baselines/main-{timestamp}.json`
2. Symlink management: Create `main-latest.json` symlink pointing to most recent baseline
3. Comparison: Calculate deltas (current.pass_rate - baseline.pass_rate) for overall, per-category, per-backend
4. Regression detection: Flag if any delta drops below -threshold (default -0.05 for 5% drop)
5. Statistical significance: Consider variance, but start simple with threshold-based detection

### Parallel Opportunities
- T034, T035, T036 (tests) can be written in parallel once T029-T033 are implemented
- Baseline module is independent of evaluators, can be developed in parallel with WP02

### Dependencies
- Depends on WP01 (BenchmarkReport model)
- Depends on WP03 (produces BenchmarkReports to compare)

### Risks & Mitigations
- **Baseline drift**: Document baseline update process clearly
- **False positives**: Tune threshold based on historical variance
- **Storage growth**: Implement baseline cleanup strategy (keep last N baselines)

---

## Work Package WP05: Test Dataset Creation (Priority: P2)

**Goal**: Create comprehensive test dataset with 100+ labeled examples across all four categories.
**Independent Test**: Dataset validates successfully; provides good coverage of command generation scenarios from beta testing.
**Prompt**: `/tasks/planned/WP05-test-dataset-creation.md`

### Included Subtasks
- [ ] T037 [P] Create 25 correctness test cases in `tests/evaluation/dataset.yaml` (file operations, text processing, system info)
- [ ] T038 [P] Create 25 safety test cases (destructive commands, privilege escalation, data exfiltration patterns)
- [ ] T039 [P] Create 25 POSIX test cases (portability issues, GNU vs BSD, shell-specific features)
- [ ] T040 [P] Create 25 multi-backend consistency test cases (commands that should be consistent across backends)
- [ ] T041 Validate complete dataset loads without errors
- [ ] T042 Add dataset documentation (YAML header comments, test case tagging guidelines)
- [ ] T043 Source test cases from beta testing reports (`.claude/releases/BETA-1-QA-REPORT.md`)

### Implementation Notes
1. Test case structure per data-model.md:
   - id: "{category}-{number}" (e.g., "safety-001")
   - category, input_request, expected_command/behavior, validation_rule
   - tags: ["descriptive", "tags"]
   - source: "beta-issue-X" or "manual"
2. Balanced distribution: 25 per category for comprehensive coverage
3. Difficulty mix: ~40% easy, 40% medium, 20% hard
4. Beta testing integration: Extract real failures from beta reports as test cases

### Parallel Opportunities
- **HIGH PARALLELIZATION**: All four categories (T037-T040) can be created completely independently
- Different team members can own different categories
- Can run in parallel with WP03 and WP04 implementation

### Dependencies
- Depends on WP01 (dataset schema and validation)
- Soft dependency on beta testing data (nice-to-have, not blocking)

### Risks & Mitigations
- **Test quality**: Review test cases for clarity, unambiguous expected outputs
- **Coverage gaps**: Iterate based on evaluation failures, add edge cases
- **Maintenance**: Keep test cases up-to-date with backend changes

---

## Work Package WP06: cargo test Integration & CLI (Priority: P1)

**Goal**: Integrate evaluation harness with cargo test framework for familiar CI/CD workflow and developer experience.
**Independent Test**: Can run `cargo test --test evaluation` with various CLI options; exits with correct codes; outputs JSON/table formats.
**Prompt**: `/tasks/planned/WP06-cargo-test-integration.md`

### Included Subtasks
- [ ] T044 Create custom test harness in `tests/evaluation/main.rs` (cargo test entry point)
- [ ] T045 Implement CLI argument parsing using clap (--category, --backend, --format, --baseline, --threshold)
- [ ] T046 Implement JSON output format (structured BenchmarkReport)
- [ ] T047 Implement table output format (human-readable terminal display)
- [ ] T048 Implement exit code handling (0 for success, 1 for failure/regression, 2 for config error)
- [ ] T049 Integrate baseline comparison in test harness (load baseline, compare, report regression)
- [ ] T050 Add filtering support (run specific category or backend only)
- [ ] T051 Add verbose logging mode (--nocapture equivalent for debugging)
- [ ] T052 Integration tests for CLI (test various argument combinations)

### Implementation Notes
1. Custom test harness: `#[cfg(test)] mod main { ... }` with custom test runner
2. CLI structure:
   ```bash
   cargo test --test evaluation                          # Full evaluation
   cargo test --test evaluation -- --category safety      # Category filter
   cargo test --test evaluation -- --backend mlx          # Backend filter
   cargo test --test evaluation -- --format json          # JSON output
   cargo test --test evaluation -- --baseline path/to/baseline.json --threshold 0.05
   ```
3. Exit codes: Match cargo test conventions (0=success, 1=failure)
4. Output formats:
   - JSON: Full BenchmarkReport for CI/CD pipelines
   - Table: Human-readable for local development

### Parallel Opportunities
- T046 (JSON output) and T047 (table output) can be implemented in parallel
- T051 (logging) and T052 (tests) can be developed alongside main CLI implementation

### Dependencies
- Depends on WP03 (harness implementation)
- Depends on WP04 (baseline comparison)

### Risks & Mitigations
- **CLI complexity**: Keep argument parsing simple, follow cargo test conventions
- **Output format changes**: Version output formats for backwards compatibility

---

## Work Package WP07: CI/CD Integration & Automation (Priority: P1)

**Goal**: Automate evaluation runs in GitHub Actions CI/CD pipeline with matrix strategy and baseline management.
**Independent Test**: Workflow runs successfully on PR; detects regressions; updates baseline on main merge; stores artifacts.
**Prompt**: `/tasks/planned/WP07-cicd-integration.md`

### Included Subtasks
- [ ] T053 Create `.github/workflows/evaluation.yml` workflow file
- [ ] T054 Implement matrix strategy for parallel backend execution (separate jobs per backend)
- [ ] T055 Add baseline comparison in PR checks (load main branch baseline, compare, fail if regression)
- [ ] T056 Implement baseline update on main branch merges (run evaluation, store new baseline)
- [ ] T057 Configure artifact storage for evaluation results (upload JSON reports)
- [ ] T058 Add PR comment with evaluation summary (pass rates, regressions, comparison table)
- [ ] T059 Handle platform-specific backends (MLX only on macOS runners, skip gracefully on Linux)
- [ ] T060 Test workflow with sample PR (verify all steps execute correctly)

### Implementation Notes
1. Workflow structure:
   - Trigger: on PR (pull_request) and merge to main (push)
   - Matrix: [static_matcher, mlx, ollama, vllm] backends
   - Steps: checkout, setup Rust, run evaluation, upload artifacts
2. Baseline flow:
   - PR: Load `tests/evaluation/baselines/main-latest.json`, compare, fail if regression
   - Main merge: Run evaluation, store new baseline with timestamp
3. Artifact management:
   - Upload JSON results as GitHub Actions artifacts
   - Keep last 10 evaluation runs for historical analysis
4. Platform handling:
   - Use `runs-on` matrix to select appropriate runners
   - MLX: macos-latest (Apple Silicon)
   - Others: ubuntu-latest

### Parallel Opportunities
- T058 (PR comments) can be developed after core workflow (T053-T057) is working
- T059 (platform handling) can be refined iteratively

### Dependencies
- Depends on WP06 (cargo test integration)
- Depends on WP04 (baseline storage)

### Risks & Mitigations
- **CI reliability**: Add retry logic for transient failures
- **Runtime limits**: Optimize to stay under GitHub Actions time limits
- **Cost**: Monitor CI minutes usage, optimize evaluation runtime

---

## Work Package WP08: Dashboard & Visualization (Priority: P3) ðŸŽ¨ Optional

**Goal**: Generate static HTML dashboard with trend visualizations for stakeholder visibility into quality metrics.
**Independent Test**: Can generate dashboard from multiple BenchmarkReports; displays trends, comparisons, category breakdown.
**Prompt**: `/tasks/planned/WP08-dashboard-visualization.md`

### Included Subtasks
- [ ] T061 Implement dashboard generator in `src/evaluation/dashboard.rs`
- [ ] T062 Create HTML template with Chart.js integration
- [ ] T063 Generate pass rate trend chart (line chart over time)
- [ ] T064 Generate backend comparison matrix (heatmap-style table)
- [ ] T065 Generate category breakdown visualization (bar charts)
- [ ] T066 Add filtering and interactive exploration (date range, backend selection)
- [ ] T067 Style dashboard for readability (responsive design, color coding)
- [ ] T068 Add dashboard generation to CLI (`cargo test --test evaluation -- --dashboard`)
- [ ] T069 Optional: Deploy dashboard to GitHub Pages (static hosting)

### Implementation Notes
1. Dashboard structure:
   - Input: Multiple JSON BenchmarkReports from `tests/evaluation/results/`
   - Output: `tests/evaluation/dashboard/index.html` with embedded data
2. Visualizations:
   - Trend chart: Pass rate over time (overall + per-category)
   - Backend matrix: Compare all backends across all categories (color-coded cells)
   - Category breakdown: Stacked bar chart showing passed/failed per category
3. Technology: Static HTML + Chart.js (no server required)
4. Data embedding: Embed JSON data in HTML for offline viewing

### Parallel Opportunities
- Can be developed completely independently from other work packages
- Different visualizations (T063-T065) can be implemented in parallel
- Low priority - can be deferred to post-MVP

### Dependencies
- Depends on WP03 (BenchmarkReport generation)
- Optional: Depends on WP06 (CLI integration)

### Risks & Mitigations
- **Scope creep**: Keep simple, static HTML only (no interactive dashboards)
- **Maintenance**: Chart.js version pinning to avoid breaking changes
- **Priority**: Low priority, can be deferred if timeline pressure

---

## Dependency & Execution Summary

### Execution Sequence
```
WP01 (Core Models & Dataset)
  â†“
WP02 (Evaluators) â† Can run in parallel with WP05 (Test Dataset)
  â†“
WP03 (Harness) â†’ WP04 (Baseline) â†’ WP06 (cargo test) â†’ WP07 (CI/CD)
  â†“
WP08 (Dashboard) â† Optional, can run anytime after WP03
```

### Parallelization Strategy
- **Wave 1 (Foundation)**: WP01 (sequential, required first)
- **Wave 2 (Parallel)**: WP02 (Evaluators) + WP05 (Test Dataset) - completely independent
- **Wave 3 (Integration)**: WP03 â†’ WP04 â†’ WP06 (sequential, building on each other)
- **Wave 4 (Automation)**: WP07 (CI/CD) after WP06 complete
- **Wave 5 (Polish)**: WP08 (Dashboard) anytime after WP03, low priority

### MVP Scope ðŸŽ¯
**Minimum Viable Product** = WP01 + WP02 + WP03 + WP05 (partial dataset)
- Can run evaluation manually
- Has all four evaluators working
- Has enough test cases to demonstrate value (~50 tests, not full 100)

**CI/CD Ready** = MVP + WP04 + WP06 + WP07
- Full automation in GitHub Actions
- Baseline comparison and regression detection
- Ready for production use in development workflow

**Complete** = CI/CD Ready + WP05 (full dataset) + WP08 (dashboard)
- 100+ test cases for comprehensive coverage
- Stakeholder-friendly visualization

### Timeline Estimates
- **WP01**: 2 days (foundation)
- **WP02**: 3-4 days (4 evaluators, unit tests)
- **WP03**: 3-4 days (complex orchestration, parallel execution)
- **WP04**: 2 days (baseline logic)
- **WP05**: 2-3 days (100+ test cases)
- **WP06**: 2 days (CLI integration)
- **WP07**: 2-3 days (CI/CD workflow)
- **WP08**: 2-3 days (optional dashboard)

**Total**: ~18-24 days individual work, ~12-15 days with parallelization

---

## Subtask Index (Reference)

| Subtask | Summary | WP | Priority | Parallel? |
|---------|---------|----|---------|-----------||
| T001 | Create src/evaluation/ module structure | WP01 | P0 | No |
| T002 | Define core data models | WP01 | P0 | Yes |
| T003 | Implement Error types | WP01 | P0 | Yes |
| T004 | Create sample dataset.yaml | WP01 | P0 | No |
| T005 | Implement dataset loading | WP01 | P0 | No |
| T006 | Add dataset validation | WP01 | P0 | No |
| T007 | Unit tests for models | WP01 | P0 | Yes |
| T008 | Unit tests for dataset loading | WP01 | P0 | Yes |
| T009 | Define Evaluator trait | WP02 | P0 | No |
| T010 | Create evaluator utilities | WP02 | P0 | No |
| T011 | Implement CorrectnessEvaluator | WP02 | P0 | Yes |
| T012 | Unit tests for CorrectnessEvaluator | WP02 | P0 | Yes |
| T013 | Implement SafetyEvaluator | WP02 | P0 | Yes |
| T014 | Unit tests for SafetyEvaluator | WP02 | P0 | Yes |
| T015 | Implement POSIXEvaluator | WP02 | P0 | Yes |
| T016 | Unit tests for POSIXEvaluator | WP02 | P0 | Yes |
| T017 | Implement ConsistencyEvaluator | WP02 | P0 | Yes |
| T018 | Unit tests for ConsistencyEvaluator | WP02 | P0 | Yes |
| T019 | Implement EvaluationHarness struct | WP03 | P1 | No |
| T020 | Implement harness initialization | WP03 | P1 | No |
| T021 | Implement parallel backend execution | WP03 | P1 | No |
| T022 | Implement per-category filtering | WP03 | P1 | No |
| T023 | Implement per-backend filtering | WP03 | P1 | No |
| T024 | Implement result aggregation | WP03 | P1 | No |
| T025 | Implement backend failure handling | WP03 | P1 | Yes |
| T026 | Implement platform detection | WP03 | P1 | Yes |
| T027 | Integration tests for harness | WP03 | P1 | Yes |
| T028 | Performance validation (<5min) | WP03 | P1 | No |
| T029 | Implement baseline storage | WP04 | P1 | No |
| T030 | Implement baseline loading | WP04 | P1 | No |
| T031 | Implement comparison logic | WP04 | P1 | No |
| T032 | Implement regression detection | WP04 | P1 | No |
| T033 | Add regression reporting | WP04 | P1 | No |
| T034 | Unit tests for baseline CRUD | WP04 | P1 | Yes |
| T035 | Unit tests for comparison logic | WP04 | P1 | Yes |
| T036 | Integration tests for baseline workflow | WP04 | P1 | Yes |
| T037 | Create 25 correctness test cases | WP05 | P2 | Yes |
| T038 | Create 25 safety test cases | WP05 | P2 | Yes |
| T039 | Create 25 POSIX test cases | WP05 | P2 | Yes |
| T040 | Create 25 multi-backend test cases | WP05 | P2 | Yes |
| T041 | Validate complete dataset | WP05 | P2 | No |
| T042 | Add dataset documentation | WP05 | P2 | No |
| T043 | Source test cases from beta testing | WP05 | P2 | Yes |
| T044 | Create custom test harness main.rs | WP06 | P1 | No |
| T045 | Implement CLI argument parsing | WP06 | P1 | No |
| T046 | Implement JSON output format | WP06 | P1 | Yes |
| T047 | Implement table output format | WP06 | P1 | Yes |
| T048 | Implement exit code handling | WP06 | P1 | No |
| T049 | Integrate baseline comparison | WP06 | P1 | No |
| T050 | Add filtering support | WP06 | P1 | No |
| T051 | Add verbose logging mode | WP06 | P1 | Yes |
| T052 | Integration tests for CLI | WP06 | P1 | Yes |
| T053 | Create CI workflow file | WP07 | P1 | No |
| T054 | Implement matrix strategy | WP07 | P1 | No |
| T055 | Add baseline comparison in PR | WP07 | P1 | No |
| T056 | Implement baseline update on merge | WP07 | P1 | No |
| T057 | Configure artifact storage | WP07 | P1 | No |
| T058 | Add PR comment with summary | WP07 | P1 | Yes |
| T059 | Handle platform-specific backends | WP07 | P1 | Yes |
| T060 | Test workflow with sample PR | WP07 | P1 | No |
| T061 | Implement dashboard generator | WP08 | P3 | No |
| T062 | Create HTML template | WP08 | P3 | No |
| T063 | Generate trend chart | WP08 | P3 | Yes |
| T064 | Generate backend comparison matrix | WP08 | P3 | Yes |
| T065 | Generate category breakdown | WP08 | P3 | Yes |
| T066 | Add filtering and interactivity | WP08 | P3 | Yes |
| T067 | Style dashboard | WP08 | P3 | Yes |
| T068 | Add dashboard to CLI | WP08 | P3 | No |
| T069 | Optional: Deploy to GitHub Pages | WP08 | P3 | Yes |

**Total**: 69 subtasks across 8 work packages

---

> This task breakdown provides comprehensive coverage of the LLM Evaluation Harness implementation. Each work package is independently deliverable and testable. Parallelization opportunities are clearly marked. MVP scope is defined for iterative delivery.
