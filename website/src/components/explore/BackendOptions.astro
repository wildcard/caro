---
const backends = [
  {
    name: "llama.cpp (Metal)",
    id: "backend-mlx",
    icon: "üçé",
    tagline: "Apple Silicon Native",
    description: "High-performance local inference using llama.cpp with Metal GPU acceleration on Apple Silicon",
    status: "available",
    highlights: [
      "Metal GPU acceleration on M1/M2/M3/M4 chips",
      "GGUF model format for efficient quantization",
      "Complete offline capability",
      "Privacy-focused (no data leaves your machine)",
      "Unified memory architecture for efficiency"
    ],
    performance: {
      startup: "< 500ms",
      inference: "< 2s",
      memory: "~2GB with 7B model"
    },
    setup: [
      "Build with: cargo build --features embedded-mlx",
      "Download a GGUF model (e.g., from Hugging Face)",
      "Configure model path in caro config",
      "Test: caro \"list files\""
    ],
    bestFor: ["macOS users with Apple Silicon", "Privacy-conscious developers", "Offline workflows", "Low-latency requirements"]
  },
  {
    name: "Exo Cluster",
    id: "backend-exo",
    icon: "üåê",
    tagline: "Distributed Inference",
    description: "Connect to Exo distributed clusters for running large models across multiple devices",
    status: "available",
    highlights: [
      "Distributed inference across multiple devices",
      "RDMA over Thunderbolt for high-speed communication",
      "Run large models (e.g., llama-3.1-405b) across clusters",
      "Automatic peer discovery on local networks",
      "OpenAI-compatible API"
    ],
    performance: {
      startup: "< 200ms (client)",
      inference: "2-5s (cluster dependent)",
      memory: "Distributed across cluster"
    },
    setup: [
      "Set up Exo cluster (github.com/exo-explore/exo)",
      "Configure endpoint: caro config set exo.url http://localhost:52415",
      "Select model: caro config set exo.model llama-3.2-3b",
      "Test connection: caro \"echo hello\""
    ],
    bestFor: ["Running large models locally", "Multi-device setups", "Apple Silicon clusters", "High-performance inference"]
  },
  {
    name: "vLLM",
    id: "backend-vllm",
    tagline: "High-Throughput Server",
    description: "Connect to vLLM servers for production-grade inference with optimized performance",
    status: "untested",
    icon: "üöÄ",
    highlights: [
      "PagedAttention for efficient memory",
      "Continuous batching for throughput",
      "Multi-GPU support",
      "OpenAI-compatible API",
      "Production-ready deployment"
    ],
    performance: {
      startup: "< 150ms (client)",
      inference: "< 5s (network dependent)",
      memory: "Server-side (client is lightweight)"
    },
    setup: [
      "Deploy vLLM server (Docker/k8s recommended)",
      "Configure endpoint: caro config set vllm.url https://api.example.com",
      "Set API key: caro config set vllm.api_key YOUR_KEY",
      "Test connection: caro \"echo hello\""
    ],
    bestFor: ["Team deployments", "Centralized inference", "High-throughput scenarios", "Cloud-based workflows"]
  },
  {
    name: "Ollama",
    id: "backend-ollama",
    icon: "ü¶ô",
    tagline: "Local Model Management",
    description: "Connect to Ollama for local LLM inference with easy model management",
    status: "planned",
    highlights: [
      "Cross-platform (macOS, Linux, Windows)",
      "Simple model management (ollama pull/list)",
      "Multiple model support (Llama, Mistral, etc.)",
      "GPU acceleration (CUDA, Metal)",
      "Active community and ecosystem"
    ],
    performance: {
      startup: "< 200ms",
      inference: "2-5s (hardware dependent)",
      memory: "Varies by model"
    },
    setup: [
      "Install Ollama from ollama.ai",
      "Pull a model: ollama pull llama2",
      "Configure Caro: caro config set backend ollama",
      "Specify model: caro config set ollama.model llama2"
    ],
    bestFor: ["Cross-platform development", "Model experimentation", "Local-first workflows", "GPU-accelerated inference"]
  },
  {
    name: "LM Studio",
    id: "backend-lmstudio",
    icon: "üñ•Ô∏è",
    tagline: "Desktop LLM Runner",
    description: "Connect to LM Studio's local server for a GUI-based local model experience",
    status: "planned",
    highlights: [
      "User-friendly desktop application",
      "Easy model discovery and download",
      "Local inference with GPU support",
      "OpenAI-compatible API server",
      "Cross-platform (macOS, Windows, Linux)"
    ],
    performance: {
      startup: "< 200ms (client)",
      inference: "2-5s (hardware dependent)",
      memory: "Varies by model"
    },
    setup: [
      "Download LM Studio from lmstudio.ai",
      "Download and load a model in LM Studio",
      "Start the local server in LM Studio",
      "Configure Caro: caro config set backend lmstudio"
    ],
    bestFor: ["GUI model management", "Easy setup experience", "Local-first workflows", "Desktop users"]
  },
  {
    name: "CPU Backend",
    id: "backend-embedded",
    tagline: "Built-in Fallback",
    icon: "üì¶",
    description: "Cross-platform CPU-based inference using Candle (currently in development)",
    status: "in-development",
    highlights: [
      "Cross-platform (macOS, Linux, Windows)",
      "No GPU required",
      "Candle-based inference engine",
      "Automatic fallback option",
      "Works on any hardware"
    ],
    performance: {
      startup: "< 50ms",
      inference: "5-15s",
      memory: "< 500MB"
    },
    setup: [
      "Build with default features",
      "Configure model path in caro config",
      "Will be used automatically as fallback"
    ],
    bestFor: ["Older hardware", "Non-GPU systems", "Emergency fallback", "Cross-platform compatibility"]
  }
];
---

<section id="backend-options">
  <div class="container">
    <h2>Choose Your Backend</h2>
    <p class="section-subtitle">Flexible inference options from ultra-fast local to scalable cloud deployments</p>

    <div class="backends">
      {backends.map((backend) => (
        <div class="backend-card" id={backend.id}>
          <div class="backend-header">
            <div class="header-left">
              <span class="backend-icon">{backend.icon}</span>
              <div>
                <h3>{backend.name}</h3>
                <p class="tagline">{backend.tagline}</p>
              </div>
            </div>
            <span class={`status-badge status-${backend.status}`}>
              {backend.status === 'in-development' ? 'In Development' :
               backend.status === 'available' ? 'Available' :
               backend.status === 'untested' ? 'Untested' : 'Planned'}
            </span>
          </div>

          <p class="backend-description">{backend.description}</p>

          <div class="backend-content">
            <div class="highlights">
              <h4>Key Features</h4>
              <ul>
                {backend.highlights.map((highlight) => (
                  <li>{highlight}</li>
                ))}
              </ul>
            </div>

            <div class="performance-metrics">
              <h4>Performance</h4>
              <div class="metrics">
                <div class="metric">
                  <span class="metric-label">Startup</span>
                  <span class="metric-value">{backend.performance.startup}</span>
                </div>
                <div class="metric">
                  <span class="metric-label">Inference</span>
                  <span class="metric-value">{backend.performance.inference}</span>
                </div>
                <div class="metric">
                  <span class="metric-label">Memory</span>
                  <span class="metric-value">{backend.performance.memory}</span>
                </div>
              </div>
            </div>

            <div class="setup-guide">
              <h4>Quick Setup</h4>
              <ol>
                {backend.setup.map((step) => (
                  <li>{step}</li>
                ))}
              </ol>
            </div>

            <div class="best-for">
              <h4>Best For</h4>
              <div class="tags">
                {backend.bestFor.map((use) => (
                  <span class="tag">{use}</span>
                ))}
              </div>
            </div>
          </div>
        </div>
      ))}
    </div>

    <div class="comparison-note">
      <h3>Which Backend Should I Choose?</h3>
      <p>
        <strong>On Apple Silicon?</strong> Use llama.cpp with Metal for fast, private, offline inference.<br/>
        <strong>Have multiple Macs?</strong> Set up an Exo cluster to run larger models across devices.<br/>
        <strong>Running a server?</strong> vLLM offers high-throughput production inference (testing welcome!).<br/>
        <strong>Prefer a GUI?</strong> LM Studio and Ollama support are planned for easier model management.
      </p>
    </div>
  </div>
</section>

<style>
  .backends {
    display: grid;
    gap: 30px;
    margin-top: 50px;
  }

  .backend-card {
    background: var(--color-bg-secondary);
    border: 2px solid var(--color-border);
    border-radius: 12px;
    padding: 30px;
    transition: transform 0.3s, box-shadow 0.3s, background 0.3s ease, border-color 0.3s ease;
  }

  .backend-card:hover {
    transform: translateY(-4px);
    box-shadow: 0 12px 36px rgba(255, 140, 66, 0.15);
    border-color: #ff8c42;
  }

  :global(.dark) .backend-card:hover {
    box-shadow: 0 12px 36px rgba(255, 140, 66, 0.25);
  }

  .backend-header {
    display: flex;
    align-items: flex-start;
    justify-content: space-between;
    margin-bottom: 20px;
    padding-bottom: 20px;
    border-bottom: 2px solid var(--color-border);
  }

  .header-left {
    display: flex;
    align-items: flex-start;
    gap: 15px;
  }

  .backend-icon {
    font-size: 48px;
  }

  .backend-card h3 {
    font-size: 24px;
    color: var(--color-text);
    margin: 0 0 5px 0;
    transition: color 0.3s ease;
  }

  .tagline {
    font-size: 14px;
    color: #ff8c42;
    font-weight: 600;
    margin: 0;
  }

  .status-badge {
    font-size: 11px;
    font-weight: 600;
    padding: 6px 12px;
    border-radius: 6px;
    white-space: nowrap;
  }

  .status-in-development {
    background: rgba(33, 150, 243, 0.1);
    color: #2196f3;
  }

  .status-available {
    background: rgba(76, 175, 80, 0.1);
    color: #4caf50;
  }

  .status-planned {
    background: rgba(156, 39, 176, 0.1);
    color: #9c27b0;
  }

  .status-untested {
    background: rgba(255, 152, 0, 0.1);
    color: #ff9800;
  }

  .backend-description {
    color: var(--color-text-secondary);
    font-size: 16px;
    line-height: 1.6;
    margin-bottom: 25px;
    transition: color 0.3s ease;
  }

  .backend-content {
    display: grid;
    gap: 25px;
  }

  .highlights h4,
  .performance-metrics h4,
  .setup-guide h4,
  .best-for h4 {
    font-size: 16px;
    color: var(--color-text);
    margin-bottom: 12px;
    transition: color 0.3s ease;
  }

  .highlights ul {
    list-style: none;
    padding: 0;
    margin: 0;
    display: grid;
    gap: 8px;
  }

  .highlights li {
    padding-left: 24px;
    color: var(--color-text-tertiary);
    font-size: 14px;
    position: relative;
    line-height: 1.6;
    transition: color 0.3s ease;
  }

  .highlights li::before {
    content: "‚Üí";
    position: absolute;
    left: 0;
    color: #ff8c42;
    font-weight: bold;
  }

  .metrics {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 15px;
  }

  .metric {
    background: var(--color-bg);
    border: 1px solid var(--color-border);
    border-radius: 8px;
    padding: 15px;
    display: flex;
    flex-direction: column;
    gap: 8px;
    transition: background 0.3s ease, border-color 0.3s ease;
  }

  .metric-label {
    font-size: 12px;
    color: var(--color-text-secondary);
    text-transform: uppercase;
    letter-spacing: 0.5px;
    transition: color 0.3s ease;
  }

  .metric-value {
    font-size: 18px;
    font-weight: 700;
    color: #ff8c42;
  }

  .setup-guide ol {
    margin: 0;
    padding-left: 20px;
  }

  .setup-guide li {
    padding: 8px 0;
    color: var(--color-text-tertiary);
    font-size: 14px;
    line-height: 1.6;
    transition: color 0.3s ease;
  }

  .setup-guide li code {
    background: var(--color-bg);
    padding: 2px 6px;
    border-radius: 4px;
    font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
    font-size: 13px;
    transition: background 0.3s ease;
  }

  .tags {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
  }

  .tag {
    background: var(--color-bg);
    border: 1px solid var(--color-border);
    padding: 6px 12px;
    border-radius: 6px;
    font-size: 13px;
    color: var(--color-text-tertiary);
    transition: all 0.2s ease;
  }

  .tag:hover {
    background: var(--color-bg-tertiary);
    border-color: #ff8c42;
    transform: translateY(-1px);
  }

  .comparison-note {
    background: linear-gradient(135deg, rgba(255, 140, 66, 0.1) 0%, rgba(255, 107, 53, 0.1) 100%);
    border: 2px solid rgba(255, 140, 66, 0.3);
    border-radius: 12px;
    padding: 30px;
    margin-top: 50px;
  }

  .comparison-note h3 {
    font-size: 22px;
    color: var(--color-text);
    margin-bottom: 15px;
    transition: color 0.3s ease;
  }

  .comparison-note p {
    color: var(--color-text-secondary);
    line-height: 1.8;
    font-size: 15px;
    margin: 0;
    transition: color 0.3s ease;
  }

  .comparison-note strong {
    color: var(--color-text);
    transition: color 0.3s ease;
  }

  @media (max-width: 768px) {
    .backend-header {
      flex-direction: column;
      gap: 15px;
    }

    .metrics {
      grid-template-columns: 1fr;
    }
  }
</style>
